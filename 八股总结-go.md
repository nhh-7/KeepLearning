## -------------Go----------------

### Goroutine与线程的区别

1. 内存消耗

   线程： 创建一个系统级线程通常需要 1MB - 2MB 的栈内存。
   Goroutine： 初始栈空间仅需 2KB 左右。它会根据需要动态地伸缩（最大可达 1GB）

2. 调度方式 (M:N 模型)
  线程： 由操作系统内核调度。当线程切换时，CPU 需要保存寄存器、更新堆栈指针等，这种“上下文切换”开销较大。Goroutine： 由 Go 运行时 (Runtime) 自行调度。Go 内部有一个名为 G-M-P 的调度模型。

3. 创建与销毁
  线程： 涉及系统调用，开销高。
  Goroutine： 纯用户态操作，创建和销毁的速度极快，几乎可以忽略不计。

### GMP 调度模型

它负责将成千上万个 Goroutine (G) 映射到有限的操作系统线程 (M) 上执行。
其核心组件如下：

- G (Goroutine)： 即协程。它保存了任务执行的上下文（栈、程序计数器 PC 等）。G 只是一个管理任务的数据结构，不直接执行。
- M (Machine)： 操作系统线程。由内核调度，是真正的执行计算资源。M 想要执行 G，必须先绑定一个 P。
- P (Processor)： 逻辑处理器（上下文）。P 维护了一个本地局部队列，里面存放着待运行的 G。
  - P 的数量决定了系统并行的上限（通常等于 CPU 核心数，可通过 GOMAXPROCS 设置）。
  - 核心作用：解耦 G 和 M。即使某个 M 因为系统调用阻塞了，P 可以带着剩下的 G 去找别的 M 执行。

调度策略的核心：

- Work Stealing：当一个 P 的本地队列空了，它会去别的 P 那里“偷”一半的 G 过来执行，防止 CPU 闲置。
- Hand Off：当 M 因为执行 G 发生了阻塞的系统调用时，M 会释放 P，把 P 转交给其他的 M（或创建新 M）去接管，保证 P 里的其他 G 不被阻塞。

###  Goroutine 发生抢占的情况

1. 基于协作的抢占（在函数调用点）

   Go 编译器会在函数的入口处插入一段名为 morestack() 的检测代码。

   - 触发：运行时如果发现某个 G 执行时间过长（超过 10ms），会给该 G 发送一个“抢占标记”。

   - 发生：当该 G 执行到下一个函数调用时，会触发堆栈检测，发现标记后主动让出 CPU 权限，挂起到就绪队列。

2. 基于信号的真抢占（非协作式，v1.14+）

   为了处理那种“连函数都不调用”的纯计算死循环，Go 引入了基于信号的抢占。

   - 触发：监控线程（sysmon）发现某个 G 运行超过 10ms。

   - 发生：内核向执行该 G 的线程（M）发送一个 SIGURG 信号。
   - 后果：M 收到信号后会被硬件中断，进而进入信号处理函数。Go 运行时会在处理函数中强制挂起当前 G，并调度下一个 G。这解决了死循环导致的程序卡死问题。

3. 系统调用（Syscall）抢占
   场景：当 G 发起一个同步系统调用（如读取大文件）。
   过程：监控线程（sysmon）会发现 P 处于 Syscall 状态。它会将 P 与当前的 M 解绑（P 重新寻找空闲的 M 执行后续 G），而旧的 M 则带着 G 慢慢等系统调用结束。这本质上也是一种对 P 资源的抢占和重分配。

### 逃逸分析

作用：确定一个变量应该分配在栈上还是堆上。
为什么需要逃逸分析？
	栈（Stack）： 分配和回收极快（仅需移动栈指针），随着函数返回自动释放，压力小。
	堆（Heap）： 需要垃圾回收（GC）来清理。如果所有变量都放堆上，GC 压力会巨大，导致程序变慢。

什么情况会逃逸？
 	1. 函数返回局部变量的指针： 如果函数返回一个局部变量的地址，该变量在函数结束后必须继续存在，因此必须分配在堆上
 	2. 接口（interface{}）类型存储
 	3. 闭包（Closure）引用：闭包函数如果引用了外部函数的局部变量，为了保证闭包在外部函数退出后仍能正常运行，这些变量会逃逸到堆。
 	4. 变量空间过大或由于动态大小：在 `make` 切片时，如果长度是**变量**而不是常量，编译器无法在编译期确定其大小，通常也会逃逸。

​	https://dablelv.github.io/go-coding-advice/%E7%AC%AC%E5%9B%9B%E7%AF%87%EF%BC%9A%E6%9C%80%E4%BD%B3%E6%80%A7%E8%83%BD/2.%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/3.%E5%87%8F%E5%B0%91%E9%80%83%E9%80%B8%EF%BC%8C%E5%B0%86%E5%8F%98%E9%87%8F%E9%99%90%E5%88%B6%E5%9C%A8%E6%A0%88%E4%B8%8A.html

如何查看逃逸分析结果？
	在执行 `go build` 或 `go run` 时，加上 `-gcflags="-m"` 参数。

### 垃圾回收机制

1. 三色标记法
   为了实现并发扫描（即 GC 和用户程序同时运行），Go 将对象分为三种颜色：
   	白色：潜在的垃圾，其内存可能会被回收。
   	灰色：活跃对象，但其引用的对象尚未被扫描。
   	黑色：活跃对象，且其引用的对象已全部扫描完成。

​	GC 过程：
​		起初所有对象都是白色。
​		从根对象（栈、全局变量）开始，将其标记为灰色。
​		遍历灰色对象，将它们引用的对象转为灰色，自身转为黑色。
​		重复此过程，直到没有灰色对象。
​		清理剩下的白色对象。

2. 混合写屏障 (Hybrid Write Barrier)

   这是 Go 1.8 引入的黑科技，它解决了在并发标记期间，用户程序修改指针导致对象丢失的问题。

减少GC停顿时间的优化思路
	本质是**减少堆内存的分配压力**和**减少扫描对象数量**。

- 减少内存分配（复用对象）
- 利用逃逸分析（让变量留在栈上）
- 避免频繁创建小对象

- 

- ​	

### error 和 panic

`error` 是一个接口。它是**业务逻辑的一部分**，代表“可能会发生且需要处理的情况”。
`panic` 类似于其他语言中的 `Unchecked Exception`。它代表程序进入了**不可恢复的非法状态**。

### 接口和泛型

1. 接口 (Interface)：解决的是“行为抽象”问题，多个类型有共同的操作
  接口定义了一组行为（方法）。它不关心对象是什么，只关心对象能做什么。

2. 泛型 (Generics)：解决的是“类型安全与代码复用”问题，逻辑相同但类型不同
泛型允许你在编写函数或数据结构时使用类型占位符，直到使用时才确定具体类型。

3. interface{} 和 any 的关系
any是别名

4. 使用泛型相比接口的优势
  虽然泛型和接口在某些场景下可以互换，但泛型有以下显著优势：

  		1. 类型安全（无需断言）
       接口：如果你把数据存入 []interface{}，取出时必须进行类型断言 v.(int)，这在运行时可能触发 panic。
       泛型：在编译阶段就确定了类型。如果你定义了 List[T]，当你创建 List[int] 时，编译器会确保你只能存入 int，取出时直接就是 int。

  		2. 更好的性能
       接口：涉及 动态派发，通常需要通过 itab 查找方法表，且往往会导致变量逃逸到堆上，增加 GC 压力。
       泛型：Go 编译器会对泛型进行 “实例化”。在编译时生成特定类型的版本（或类似机制）。它更接近于静态调用，对编译器优化更友好，减少了运行时的装箱/拆箱开销。

  		3. 某些功能实现更加复杂
       例子：写一个 Max(a, b T) T 函数。

       ​	用接口很难实现，因为你需要定义一个 Comparable 接口，而且返回值往往需要类型断言回具体类型。
       ​	用泛型非常简单：`func Max[T constraints.Ordered](a, b T) T { ... }`。它能保证输入和输出的类型是完全一致的。

### goroutine泄露

​	是指你启动了一个 Goroutine，但由于某种原因（如逻辑阻塞或死循环），它**永远无法结束**，也无法被垃圾回收（GC）销毁。

场景：

1. Channel 导致的阻塞
   	发送者阻塞：向一个没有接收者的无缓冲 Channel 发送数据。
   	接收者阻塞：从一个永远不会有数据存入、也永远不会关闭的 Channel 接收数据。

2. Select 语句没有 Default 分支

3. Goroutine 试图获取一个永远不会被释放的锁。

4.  循环中的 Goroutine 未退出

   ​	例如，在一个 `for` 循环中启动了监听任务，但没有设置退出机制（如 Context 取消）。

### context的应用场景

​	context包是管理并发控制、生命周期和跨层级传递数据的标准方式。它主要解决了 “如何安全、高效地通知成千上万个 Goroutine 停止工作” 的问题。

1. 超时控制 (Timeout)
   在访问数据库、调用第三方 API 或执行耗时操作时，我们不能让请求无限期等待。

   ​	用法：context.WithTimeout

2. 取消信号传递 (Cancellation)
   当一个父级请求因为某种原因（如用户关闭了浏览器）被取消时，该请求派生出的所有下游 Goroutine 都应该立即停止执行，以节省资源。

   ​	场景：主请求取消后，停止相关的数据库查询和缓存写入。

   ​	用法：context.WithCancel

3. 截止时间控制 (Deadline)
  与超时类似，但它的是一个具体的时间点（Time object），而不是时间段（Duration）。

  ​	用法：context.WithDeadline

4. 跨层级元数据传递 (Value)
  在请求的生命周期内，在不同的函数、中间件或服务层之间传递一些公共信息。

  ​	场景：传递 TraceID（链路追踪）、UserID（当前登录用户）、Token。

  ​	用法：context.WithValue

  ​	注意：不要用它传递业务参数。它应该是与业务逻辑无关、用于横向关注点（如日志、监控）的数据。

注意事项：

1. 不要将 Context 存储在结构体中：Context 应该作为函数的第一个参数显式传递，变量名通常叫 ctx。
2. 不要传递 nil Context：如果你不确定要传什么，使用 context.TODO()，而不是 nil。
3. Context 是不可变的（Immutable）：每次衍生（WithCancel, WithValue 等）都会返回一个新的子 Context。当**父级被取消时，所有子级都会收到信号**。
4. 及时调用 Cancel（除了WithValue之外的创建的方式会返回Cancel函数）：无论是超时还是正常结束，只要使用了派生的 Context，一定要记得 defer cancel()，否则会导致 Goroutine 泄露或计时器无法释放。

### go的内存模型

​	Go 的内存模型规定了：**“在一个 Goroutine 中对变量进行写入，在什么条件下能保证被另一个 Goroutine 观察到。”**

#### Happens Before

​	Happens-Before 是一种的“可见性保证”。如果事件 E_1 Happens-Before 事件 E_2，那么 E_2 的执行者一定能看到 E_1 产生的所有结果（比如对变量的赋值）。只有符合规则才能**确定代码执行顺序**

在 Go 中，以下是一些典型的 Happens-Before 规则：

1. 单协程规则：在同一个 Goroutine 中，代码逻辑上的顺序就是 Happens-Before 顺序。
2. Channel 规则（核心）：
   1. 发送 Happens-Before 接收完成。
   2. Channel 关闭 Happens-Before 接收到零值。
   3. 对于无缓冲 Channel，接收完成 Happens-Before 发送完成（强制同步）。
3. 锁规则：第 n 次 Unlock Happens-Before 第 n+1 次 Lock。
4. 启动规则：go 语句的执行 Happens-Before 该 Goroutine 内部的第一行代码。

#### Don't communicate by sharing memory; share memory by communicating

传统方式：通过共享内存来通信 (Share memory to communicate)

- 做法：两个线程通过读写同一个变量（如全局变量、Map）来交换信息。
- 代价：为了防止竞争，必须频繁加锁（Mutex）。
- 痛点：锁竞争（Lock Contention）会导致性能下降，且容易产生死锁（Deadlock）或竞态条件（Race Condition）。

Go 的方式：通过通信来共享内存 (Share memory by communicating)

- 做法：通过 Channel 发送数据。当一个 Goroutine 将数据的指针发给另一个协程后，它就不再操作这个指针了。
- 核心逻辑：数据的**所有权发生了转移。**
- 优势：
  - 天然同步：Channel 内部保证了 Happens-Before。
  - 降低心智负担：不需要在业务逻辑里到处考虑 Lock/Unlock。
  - 解耦：生产者和消费者不需要知道对方的存在，只需要关注 Channel。

### slice实现

```go
type slice struct {
    array unsafe.Pointer // 指向底层数组起始地址的指针
    len   int            // 切片的长度（当前存储的元素个数）
    cap   int            // 切片的容量（从指针处开始，底层数组的大小）
}
```

扩容机制（以 Go 1.18+ 为准）
当你使用 append 且容量不足时，Go 会触发扩容逻辑：

- 小容量：如果期望容量小于 256，则直接翻倍（2x）。
- 大容量：如果期望容量超过 256，则不再简单翻倍，而是按照公式 newcap += (newcap + 3*256) / 4 逐渐增长。
- 内存对齐：计算完大致容量后，Go 还会根据内存分配器的规格进行向上取整（内存对齐），所以最终分配的 cap 往往比计算出的稍微大一点。

### map实现

​	 Map 使用了**哈希表实现，采用链地址法**（通过 Bucket 桶）来解决哈希冲突。

```go
type hmap struct {
    count     int    // 元素个数
    B         uint8  // 桶的数量，数量为 2^B
    buckets   unsafe.Pointer // 指向数组（桶）的指针
    oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针（用于渐进式搬迁）
    extra     *mapextra      // 溢出桶相关
}
```

1. 桶的内部 (bmap)
   每个桶（Bucket）最多存储 8 个键值对。
   - 为了内存对齐，桶内部先存储 8 个 Key，再存储 8 个 Value。
   - 桶内还有一个 tophash 数组（8 个字节），记录了每个 Key 哈希值的高 8 位，用于快速定位 Key。
   - 如果一个桶存满了（8 个全满），会通过 overflow 指针指向一个溢出桶。

2. 扩容逻辑
   Map 的扩容是渐进式的（每次进行写或删操作时搬迁 1-2 个桶），避免瞬间性能抖动。
   - 翻倍扩容：当装载因子（Load Factor）超过 6.5 时（即元素太多），桶的数量翻倍。
   - 等量扩容：当溢出桶过多但元素不多时，会进行整理，让数据更紧凑。

### nil与空值的区别

1. 什么是空值 (Zero Value)？
  在 Go 中，当你声明一个变量但不初始化它时，Go 编译器会自动赋予该变量一个默认值，这就是 空值。

  每种基础类型都有自己的空值：

  - 数值类型 (int, float, etc.): 0
  - 布尔类型 (bool): false
  - 字符串类型 (string): "" (空字符串)
  - 复数类型: 0+0i

  重点： 对于上述基础类型，它们永远不会是 nil。

2. 什么是 nil？
  nil 是 Go 中的一个预定义标识符，代表指针、接口、切片、映射、通道和函数的“零值”。

  它表示该变量目前没有指向任何底层的内存地址或数据结构。

  可以为 nil 的类型：

  - 指针 (*int, *Struct)
  - 切片 ([]int)
  - 映射 (map[string]int)
  - 通道 (chan int)
  - 接口 (interface{} / any)
  - 函数 (func())

​	一个接口变量包含两个部分：Type（类型）和 Value（值）。 只有当 Type 和 Value **同时为 nil** 时，接口才等于 nil。

### 工具-trace, pprof, benchmark

### 函数是值传递还是引用传递

​	所有的函数传参都是值传递

切片传入函数，进行append，外部是看不到变化的，因为外部的len没有变化。

1. 
2. 。
3. 函数是值传递还是引用传递
4. 缓冲、无缓冲 chan的区别
5. 如何实现deep copy
6. init的执行顺序
7. 子协程panic为何没法被父协程recover
8. Finalizer的作用
9. timer.Ticker是否精准
10. go的对象池方案及解决的问题



## -------------计算机网络----------------

1. tcp/udp区别， http协议描述， ip协议是什么
2. http和https的客户端和服务器区别
3. 为什么握手是三次而不是两次？为什么挥手是四次?
4. TCP的 TIME WAIT状态有什么作用？如果服务器上有大量 TIME_WAIT状态的连接，应该如何优化？
5. TCP的流量控制和拥塞控制有什么区别？
6. HTTP/1.1、HTTP/2、 HTTP/3的主要区别是什么?
7. 简单描述一下HTTPS的工作原理,特别是TLS握手过程。
8. RESTful API和 RPC(如gRPC)有什么区别?在什么场景下应该如何选择？

### 三次握手

- 客户端发送SYN报文，初始化序列号，进入SYN_SEND
- 服务端发送 SYN+ACK报文，初始自己的序列号，进入SYN_RECV
- 客户端发送ACK报文，客户端进入ESTABLISHED，服务端收到这个ACK后，也进入ESTABLISHED
- 四次多余，两次会导致服务端资源的浪费，每收到一个SYN就建立一个连接

### 四次挥手

- 客户端发送FIN报文，进入FIN-WAIT-1
- 服务端回复ACK，服务端进入CLOSE-WAIT，客户端收到后进入FIN_WAIT-2
- 服务端可继续发送数据
- 服务端发送FIN，进入LAST-ACK
- 客户端回复ACK，客户端进入TIME-WAIT，服务端收到后进入CLOSE

- 为什么要四次？
  - TCP 是全双工协议，需分别关闭两个方向的连接。
  - 被动关闭方可能需要时间处理剩余数据，不能立即发送 `FIN`。

- 为什么客户端要等待2MSL？MSL - 报文最大生存时间
  - 确保最后一个ACK报文能够到达服务器，ACK如果丢失，服务器收不到ACK，超时重传，客户端收到后重新发送
  - 确保在本连接持续期间所产生的所有报文段都从网络中消失，从而避免新旧连接之间的干扰‌

### 流量控制

- 防止发送方**发送速率过快**导致接收方缓冲区溢出。
- **核心机制**：滑动窗口 -  接收方通过 TCP 报文段中的**窗口字段（Window Size）** 告知发送方自己当前的接收缓冲区可用空间，发送方据此限制发送数据的大小。

### 拥塞控制

避免网络因**过量数据**导致拥堵，通过动动态调整发送方的**拥塞窗口**平衡全局负载， 

- 慢启动 -- 收到一个ACK，拥塞窗口就指数增长，超过阈值，进入下一阶段
- 拥塞避免--线性增长，
- 快速重传--收到三个重复ACK，立即重传丢失的报文，无需等待超时。
- 快速恢复 -- 重传后，将阈值设为当前拥塞窗口 的一半。拥塞窗口设为阈值，并进入拥塞避免阶段

### TCP与UDP对比

- TCP面向连接，UDP无连接
- TCP可靠传输，通过握手、确认、重传、拥塞控制机制。UDP不可靠传输、不保证数据不丢失
- TCP有状态、会记录消息是否发送、是否被接收等、UDP无状态
- TCP面向字节流、UDP面向报文
- TCP传输效率低
- TCP首部开销 20~60字节，比UDP首部 8 字节大
- TCP只支持点对点通信，UDP支持广播或多播
- 使用场景
  - TCP：要求通信数据可靠场景 -- 网页浏览、文件传输、邮件传输
  - UDP：要求通信速度⾼场景  --  域名转换、视频直播、实时游戏

### TCP确保可靠

- 为每个数据包指定序号
- 校验和，校验首部和数据部分，接收端计算检验和，若改变了，则丢弃
- 流量控制：TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据
- 拥塞控制
- 确认应答
- 超时重传

### KeepAlive

- TCP的Keepalive：⼀种⽤于在 TCP 连接上检测空闲连接状态的机制，就是TCP有⼀个定时任务做倒计时，超时后会触发任务，内容是发送⼀个探测报⽂给对端，⽤来判断对端是否存活。  
- HTTP的Keep-Alive：用于设置开启长连接

### 强缓存和协商缓存

- 强缓存：服务端返回资源时带上一个过期时间字段，当客户端再次访问资源，浏览器判断是否已经过期，未过期可直接读取缓存
- 协商缓存：客户端第一次访问资源，服务端会根据该文件生成一个表示文件是否改动了的标志（last-modified返回文件修改时间，`ETag`返回文件哈希值），客户端将资源缓存，后续客户端访问该资源时，会先发送该标志给服务端，服务器判断资源是否修改，未修改客户端可直接读取缓存。

### HTTP迭代

- HTTP 0.9 ：只支持GET方法，没有请求头
- HTTP 1.0 ：引入请求头和响应头，只能短连接，每次请求都要建立一个TCP连接
- HTTP 1.1 ： 支持长连接，管道化使得请求能够并行发送（也就是发送请求后，不必收到回应就继续发送请求），Host字段在同一个`IP`地址上承载多个域名
- HTTP 2 ：基于HTTPS，将数据分割为二进制帧进行传输，多路复用 -- 同一个HTTP连接发起多个请求-响应，首部压缩（`HPACK`算法），服务器可以自主向客户端推送资源
- HTTP 3 ：基于 `QUIC` 协议构建，使用 `UDP` 作为传输层协议，HTTP/3 继承并优化了 HTTP/2 的多路复用特性

### 如何建立HTTPS连接 

1. 客户端发送请求
2. 服务端生成一对公私钥，公钥传给CA机构，CA机构使用自己的私钥对服务器公钥加密，生成数字证书
3. 服务器将数字证书传回客户端
4. 客户端浏览器解析数字证书，得到服务端的公钥
5. 客户端随机生成一个密钥，用服务器公钥加密，传给服务器
6. 服务器使用自己的私钥解密，得到客户端的密钥
7. 两端使用这个密钥进行加密解密通信

### GET和POST请求的区别  

- GET⽤于从服务端获取资源， POST用来向服务器端提交数据
- GET请求的参数一般写在URL中，POST请求参数放在请求体
- 安全性不同
- GET 请求会被浏览器主动cache，⽽ POST 不会，除⾮⼿动设置。  

### 在浏览器中输入URL

结合网络模型：

1. 浏览器根据 URL 确定要访问的资源，并构建 HTTP 请求报文
2. 浏览器根据URL解析出域名，并检查缓存中是否有对应IP地址，没有则向 DNS 服务器发送域名解析请求，将域名转换为 IP 地址。
3. TCP开始三次握手，将应用层的 HTTP 请求报文封装成 TCP 报文段，添加源端口和目的端口等信息。如果是 HTTP/3 版本，会使用 UDP 协议并结合 QUIC 等机制来提高性能。
4. 将 TCP 报文段（或 UDP 数据报）封装成 IP 数据包，添加源 IP 地址和目的 IP 地址。
5. 构建数据包时，已知目标 IP 地址。通过查找本地路由表，判断目标 IP 是否在本地子网。若不在，选择默认网关作为中间跳点。
6. 将 IP 数据包封装成帧，添加源 MAC 地址和目的 MAC 地址等链路层头部信息，根据ARP协议，通过下一跳IP地址得到MAC地址。

不结合分层模型：

1. 浏览器会解析出协议、主机、端⼝、路径等信息，并构造⼀个HTTP请求  
2. DNS域名解析, 将域名解析成对应的IP地址  
3. 建⽴起TCP连接之三次握⼿  
4. 浏览器发送HTTP/HTTPS请求到web服务器  
5. 服务器处理HTTP请求并返回HTTP报⽂  
6. 浏览器渲染⻚⾯  
7. 断开连接之TCP四次挥⼿ 

### DNS查询过程 -- 客户端向本地域名服务器发起递归查询，其余过程都是迭代查询

1.  浏览器缓存
2.  本地的host文件
3.  向本地DNS服务器发送查询请求
4.  本地DNS服务器向根域名服务器查询，根域名服务器不进行解析，而是告知向哪个顶级域名服务器继续查询
5.  本地DNS服务器继续向顶级域名服务器查询，后者告知向哪个权威域名服务器查询
6.  本地DNS服务器向该权威域名服务器继续查询，得到对应IP地址。

### CDN

全称为内容分发⽹络 （Content Delivery Network） , 通过将内容存储在分布式的服务器上，使⽤户可以从距离较近的服务器获取所需的内容，从⽽减少数据传输的时间和距离，提⾼内容的传输速度、减少延迟和提升⽤户体验。  

### SYN攻击

攻击者伪造不同IP地址的SYN报⽂请求连接，服务端收到连接请求后分配资源，回复ACK+SYN包，但是由于IP地址是伪造的，⽆法收到回应，久⽽久之造成服务端半连接队列被占满，⽆法正常⼯作。  

## -------------数据结构----------------

1. 哈希表(Hash Table/Map)是如何工作的？如何解决哈希冲突? Go的map是如何实现的?它在并发环境下是安全的吗？
2. 什么是跳表(Skip List)？它解决了什么问题?
3. 什么是二叉搜索树(BST)？它有什么问题？如何改进?
4. 什么是一致性哈希？它主要解决了什么问题?

## -------------操作系统----------------

1. 进程、线程和协程是什么?它们之间有什么区别和联系？什么场景下会选择多线程?什么场景下会考虑使用协程?
2. 描述一下堆和栈的区别。函数调用是如何使用栈的？为什么堆上的内存分配比栈慢?
3. 什么是死锁？产生死锁的四个必要条件是什么?如何预防或避免死锁？
4. 什么是虚拟内存?它解决了什么问题？什么是TLB(快表)？它的作用是什么？为什么需要多级页表？
5. I/O模型有哪些？阻塞(Blocking)、非阻塞(Non-blocking)、同步(Synchronous)、异步(Asynchronous)之间有什么区别？ select/pollepoll的区别是什么? epoll 的优势在哪里(LT/ET模式)？
6. 解释一下Reactor和 Proactor两种/O设计模式。
7. 常见的 CPU 调度算法有哪些?各自适用于什么场景？
8. 什么是CPU缓存一致性？可以简单解释一下 MESI协议吗？什么是“伪共享(False Sharing)?它会对性能产生什么影响？在代码层面如何避免？
9. MMU的功能和作用
10. 什么是“缺页异常”?
11. 自旋锁和互斥锁的差别

### 操作系统进程通信

- 管道 -- 仅用于父子进程
- 命名管道 -- 允许⽆亲缘关系进程间的通信。  
- 消息队列 -- 消息队列是消息的链表，存放在内核中并由消息队列标识符标识  
- 共享内存  --  就是映射⼀段能被其他进程所访问的内存，这段共享内存由⼀个进程创建，但多个进程都可以访问。
- 信号量
- 信号
- 套接字 -- 主要⽤于在客户端和服务器之间通过⽹络进⾏通信。

### 操作系统内存管理

- 内存分配方式

  - 分页
  - 分段
  - 段页式

- 虚拟内存 -- 为进程提供**独立虚拟地址空间**，物理内存作为缓存，磁盘作为后备存储。

  - 页面置换算法

    最优置换

    先进先出

    最近最少使用

    时钟算法

  - 缺页中断

### 进程同步与异步

- 同步--互斥锁、条件变量、原子操作
- 异步
  - std::future -- 分离任务执行与结果获取，避免阻塞主线程。
  - 异步回调--任务完成后触发回调函数

### 进程、线程、协程

- 进程：操作系统资源分配的基本单位，包含独立的地址空间、代码段、数据段和系统资源（如文件句柄、内存空间）
- 线程：进程内的执行单元，共享进程的地址空间和系统资源（如内存、文件句柄）。线程是 CPU 调度的基本单位，同一进程内的线程可并发执行，减少了进程间切换的开销（进程切换需要切换地址空间、页表等操作、开销大）。
- 协程：又称 “用户态线程”，是由用户空间管理的轻量级执行单元。适合于高并发IO场景。

协程相比于线程的优势：

- 协程在用户态调度，开销小
- 协程的内存占用极小
- 线程需要操作系统内核维护线程控制块、调度队列等资源，而协程不占用内核资源。

### 读写锁的实现

允许多个线程同时读取共享资源（读锁可重入），但在写入时需要独占访问（写锁排他）。这种设计提高了并发性能，适合读多写少的场景。

- **状态跟踪**：通常使用一个计数器记录当前读者数量，以及一个标志位表示是否有写者等待或持有锁。
- **互斥锁**：保护内部状态变量的访问，确保操作的原子性。
- **条件变量**：用于线程间的通知机制，当写锁释放时通知等待的读者 / 写者。
- **读锁获取**：如果没有写者持有锁或等待，允许获取读锁并增加读者计数。
- **读锁释放**：减少读者计数，若减为 0 则通知可能等待的写者。
- **写锁获取**：等待直到没有读者和写者，然后标记写锁被持有。
- **写锁释放**：标记写锁释放，并通知所有等待的读者和写者。

### 内存对齐

内存管理中的一种优化技术，它确保数据在内存中的起始地址是特定值的整数倍

- 结构体的对齐值：取其数据成员中的最大对齐值
- CPU通常以固定大小的块（4字节、8字节）访问内存，如果未对齐，我们要用访问两次内存的值才能拼接成结果

### 僵尸进程与孤儿进程

- 僵尸进程：进程已终止后，但父进程尚未调用`wait()`或`waitpid()`获取其退出状态，此时进程描述符（PCB）仍保留在系统中，成为僵尸进程。父进程终止后，僵尸进程会被 init 接管并清除。
- 孤儿进程：父进程先于子进程终止，导致子进程成为 “孤儿”，此时子进程会被**init 进程（PID=1）或 systemd 进程**收养。

### 父子进程共享什么资源

- 代码段
- 文件描述符
- 信号处理函数
- 继承父进程的权限

### 没有虚拟内存的问题

- 每个进程必须完全加载到物理内存中，若程序所需内存超过物理 RAM 容量，则无法运行
- 系统同时运行的进程总数受限于物理内存大小
- 进程地址空间不隔离：每个进程直接访问物理内存，一个进程可通过错误指针覆盖其他进程的数据，导致系统不稳定。
- 虚拟内存通过页表实现内核空间与用户空间的隔离，无虚拟内存时，用户程序可能直接访问内核区域。

### 内部碎片与外部碎片

- 内部碎片：当内存分配的空间大于进程实际需要的空间时，多余的部分无法被其他进程使用，从而形成 “内部” 的空闲碎片。
- 外部碎片：内存中存在多个分散的空闲内存块，但它们的总大小足够满足进程需求，却因不连续而无法分配

### 用户态与核心态

- ⽤户态：在⽤户态下，进程或程序只能访问受限的资源和执⾏受限的指令集，不能直接访问操作系统的核⼼部分，也不能直接访问硬件资源。
- 核⼼态：核⼼态是操作系统的特权级别，允许进程或程序执⾏特权指令和访问操作系统的核⼼部分。在核⼼态下，进程可以直接访问硬件资源，执⾏系统调⽤，管理内存、⽂件系统等操作。
- 发生系统调用、异常、中断时、从用户态切换到核心态

### 产生死锁的条件

- 互斥条件：⼀个进程占⽤了某个资源时，其他进程⽆法同时占⽤该资源。
- 请求保持条件：⼀个线程因为请求资源⽽阻塞的时候，不会释放⾃⼰的资源。
- 不可剥夺条件：资源不能被强制性地从⼀个进程中剥夺，只能由持有者⾃愿释放。
- 循环等待：多个进程之间形成⼀个循环等待资源的链  

### 解除死锁：

- 破坏请求与保持条件：⼀次性申请所有的资源。
- 破坏不可剥夺条件：占⽤部分资源的线程进⼀步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
- 破坏循环等待条件：靠按序申请资源来预防。让所有进程按照相同的顺序请求资源，释放资源则反序释放。

## -------------数据库----------------

### MySQL索引

索引是一种数据结构，用于提高数据库表中数据的查询效率。它就像一本书的目录，通过特定的键值来快速定位到相应的数据行。

**作用**：能大大减少数据库在查询时需要扫描的数据量，从而加快查询速度。例如在一个有大量记录的用户表中，若经常根据用户名查询用户信息，为用户名字段建立索引，查询时就可直接定位到对应记录，而无需全表扫描。

**类型**：

- 数据结构角度：常见的有 B - Tree 索引、哈希索引等。B - Tree 索引适用于范围查询和排序操作，哈希索引则在等值查询时性能出色。
- 物理存储角度：聚集索引 -- 数据与索引一起存放，找到了索引就找到了数据。 非聚集索引 -- 数据存储与索引分开存放
- 逻辑角度：
  - 主键索引 -- 不允许有空值，特殊的唯一索引
  - 唯一索引 -- 索引列中的值必须是唯一的，可以为空值
  - 联合索引 -- 多个字段创建的索引， 使用时遵循 最左前缀原则

### 为什么使用B+树做索引

- B+ 树的⾮叶⼦节点不存放实际的记录数据，仅存放索引，数据量相同的情况下，B+树的⾮叶⼦节点可以存放更多的索引，整个树的高度会矮一点，查询底层节点的磁盘 I/O次数会更少。  
- B+ 树所有叶⼦节点间有⼀个链表进⾏连接，范围查询效率高

### 什么时候需要创建索引

- 表的主键字段，会自动建立唯一索引
- 经常用于WHERE查询条件的字段
- 查询中排序的字段
- 与其他表有关联的字段

### 索引失效的场景

- 对索引使用函数
- 对索引使用左 或者 左右模糊匹配
- 对索引进行表达式计算
- 对索引隐式类型转换
- OR 前的条件列是索引列，而在 OR 后的条件列不是索引列

### MySQL事务

事务是由一组 SQL 语句组成的逻辑单元，这些语句要么全部执行成功，要么全部不执行，以保证数据库的一致性。

- **特性**：具有原子性（通过undo log）、一致性、隔离性（通过MVCC，多版本并发控制）和持久性（通过redo log），即 ACID 特性。原子性确保事务中的操作要么都完成，要么都不做；一致性保证事务执行前后数据库的完整性约束得到满足；隔离性防止并发事务之间相互干扰；持久性保证事务一旦提交，其结果就永久保存在数据库中。
- 事务的隔离级别
  - 读未提交 -- 允许事务读取其他事务尚未提交的数据。--  会出现脏读，即一个事务读取到了另一个未提交事务修改的数据
  - 读提交 -- 只能读取已经提交的数据，避免了脏读。--  在一个事务内的多次查询可能会出现不可重复读的情况，即由于其他事务在该事务两次查询之间对数据进行了修改并提交，导致两次查询结果不一致。
  - 可重复读 -- 确保在一个事务内多次读取同一数据时，结果是一致的，解决了不可重复读问题。-- 可能会出现幻读，在一个事务内多次查询某个符合查询条件的「记录数量」，出现前后两次查询到的记录数量不一样的情况
  - 串行化 -- 事务串行执行，就像没有并发一样，避免了所有并发问题。-- 性能开销大

### MVCC机制

⽤于管理多个事务同时访问和修改数据库的数据，⽽不会导致数据不⼀致或冲突。MVCC的核⼼思想是每个事务在数据库中看到的数据版本是事务开始时的⼀个快照，⽽不是实际的最新版本。这使得多个事务可以并发执⾏，⽽不会互相⼲扰 

快照包含四个信息：

- m_ids 当前活跃的所有事务id（所有未提交的事务）
- min_trx_id 当前活跃事务中id最小的，事务id比这个小就代表，再快照开启前，该事务已经提交
- max_trx_id 下⼀个将要分配的事务id（版本链头事务id+1），如果一条记录的事务id**大于等于**这个就代表，快照创建后，该事务才开启，不能访问。（可以查找undo log寻找上一个修改该记录的事务）
- creator_trx_id 创建这个ReadView的事务的id 查询规则

### MySQL锁机制

锁是数据库管理系统用于控制并发访问的一种机制，它允许在同一时间内对特定的数据资源进行排他性或共享性的访问控制。

- **从操作类型上**可分为共享锁（读锁）和排他锁（写锁）。共享锁允许事务对数据进行读操作，多个事务可同时获取共享锁来读取数据；排他锁则用于写操作，一个事务获取排他锁后，其他事务不能再获取该数据的任何锁，直到排他锁被释放。
- **从粒度上**可分为表级锁、行级锁等。表级锁对整个表进行锁定，开销小但并发度低；行级锁只对特定行进行锁定，能支持更高的并发，但开销较大。
- 行锁分为：
  - 记录锁
  - 间隙锁
  - Next-Key Lock，锁定一个范围，并锁定记录本身

### Redis持久化

- AOF ⽇志：每执⾏⼀条写操作命令，就把该命令以追加的⽅式写⼊到⼀个⽂件⾥。Redis刚启动时，会读取该日志来构建数据库
- RDB 快照：将某⼀时刻的内存数据，以⼆进制的⽅式写⼊磁盘；每次执⾏快照，都是把内存中的「**所有数据**」都记录到磁盘中  
- 混合持久化⽅式： Redis 4.0 新增的⽅式，集成了 AOF 和 RBD 的优点；

## -------------设计模式----------------

### 单例模式

```C++
class Singleton {
public:
    // 删除拷贝构造函数和赋值操作符
    Singleton(const Singleton&) = delete;
    Singleton& operator=(const Singleton&) = delete;

    // 全局访问点
    static Singleton& getInstance() {
        static Singleton instance; // C++11 保证静态局部变量初始化线程安全
        return instance;
    }

private:
    Singleton() {} // 私有构造函数，使得只有类内部可创建实例
    ~Singleton() {} // 私有析构函数
};
```

双重检查锁定

```C++
class Singleton {
public:
    // 删除拷贝构造函数和赋值操作符
    Singleton(const Singleton&) = delete;
    Singleton& operator=(const Singleton&) = delete;

    // 全局访问点
    static Singleton* getInstance() {
        if (instance == nullptr)
        {
            std::lock_guard<std::mutex> lock(mutex_);
            if (instance == nullptr)
            {
                instance = new Singleton();
            }
        }
        return instance;
    }

private:
    Singleton() {} // 私有构造函数
    ~Singleton() {} // 私有析构函数
    static Singleton *instance;
    static std::mutex mutex_; // 保证无论通过多少个线程调用getInstance()，始终操作同一个mutex_
};

// 静态成员变量需要在类外初始化
Singleton::instance = nullptr;
std::mutex Singleton::mutex_;
```



### 工厂模式

将对象的创建逻辑封装在一个工厂类或工厂方法中，使代码与具体对象的依赖关系解耦

- **解耦对象的创建与使用**，通过统一接口创建不同类型对象
- 简单工厂 --  一个工厂类根据工厂函数的参数创建不同产品
- 工厂方法 --  抽象工厂定义一个创建对象的抽象方法，由具体工厂子类实现该方法来创建产品。
- 抽象工厂 --  工厂基类定义一个创建一系列相关产品的抽象接口，具体工厂（一系列相关产品的不同生厂商）实现该接口来创建产品族。



### Reactor模式

 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**

- 单Reactor单线程
  - Reactor对象通过Select监听事件，如果是连接建立事件，分发给Acceptor对象进行处理，如果是其他事件，交给handler对象进行处理
- 单Reactor多线程
  - reactor对象监听事件，根据事件类型发送给Acceptor对象或者handler对象
  - handler对象会将raad读取到的数据交给子线程进行处理，处理完后子线程会返回结果给handler，handler完成对client的响应。
- 多Reactor多线程
  - 主线程的MainReactor监听**连接建立事件**, 收到事件后通过Acceptor获取连接，将连接分配给某个子线程
  - 子线程的SubReactor将该连接加入IO多路复用进行监听，
  - 该连接有事件发生时，子线程对事件进行处理

### Proactor模式

异步网络模式，感知的是已完成的读写事件



### 异步日志具体介绍

日志库大体分为前端和后端两部分，前端是供应用程序使用的接口，并生成日志消息，后端负责把日志写到文件。每个线程都有自己的前端，整个程序共用一个后端。

如何应对若程序崩溃，那么最后若干日志丢失

- 定期将缓冲区内的日志刷新到硬盘
- 每条内存中的日志消息都带有函数地址

 基本思想：

- 准备两块Buffer：A、B，前端负责往bufferA中填数据，后端负责将bufferB的数据写入文件。当bufferA写满之后，交换A和B，让后端将bufferA的数据写入文件，而前端则往bufferB填入新的日志消息。

实际实现：

- 前端后端各持有两个缓冲区，前端和后端各有一个缓冲区数组，初始时是空的。
- 后端线程被唤醒的条件为：1.超时  2.前端写满了一个或多个buffer
- 后端线程被唤醒后，将前端正在写入的buffer加入到数组中，并将后端空闲的buffer移为前端的写入buffer。后端的缓冲区数组和前端的缓冲区数组交换过后，即可退出临界区，后端继续进行写入文件操作。

### 动态缓冲区具体介绍

Buffer类中记录了读取数据的起始位置、可继续写入的位置。底层使用vector<char>实现。通过这些vector的size以及记录的两个索引，可以计算出可写空间是多少，可读数据是多少。

input Buffer : 从socket读取数据，然后写入input buffer，客户从input buffer读取数据

output Buffer ：客户把数据写入output buffer， 连接从output buffer读取数据写入socket

自动增长策略：需要写入的数据超过当前可用空间时，缓冲区会自动增长

- 首先检查已读数据前面（也就是已经被读过了的数据，readIndex之前）是否有足够的空间
- 如果不足，会对buffer进行resize。
- 如果足够，则移动数据，使得buffer后段留出足够写入的连续空间。

buffer：readFd 

​	要读取的tcp数据大小不可控，使用栈的额外空间

- 栈上准备一个64KB的buf，利用readv读取数据，iovec有两块，第一块指向Buffer的writable字节，第二块指向栈空间，如果读入的数据不多，全部读到Buffer中了，如果数据大于了Buffer的可写空间，那么读入栈上，之后再使用栈上的数据加入到Buffer中。

作用：

- 解决 TCP 粘包 / 拆包问题；通过Buffer存储完整的数据包，等待应用程序按协议解析
- 减少read()`/`write()系统调用次数
- 内存管理优化；读写操作后通过调整索引（`readerIndex`和`writerIndex`）重用内存

### 什么是 one loop per thread，优势是什么

一种在并行计算和多线程编程中的设计模式。它的核心思想是为每个线程分配一个独立的循环任务，使得每个线程专注于处理自己的循环逻辑，从而实现任务的并行执行。

- 每个 `EventLoop` 线程负责处理自己的 I/O 事件，减少线程同步开销
- 每个 `EventLoop` 绑定一个独立的 I/O 多路复用器（如 Linux 的 epoll），减少了单个 epoll 实例的压力。

如何实现：

- 创建loop时记录当前线程 ID，通过`isInLoopThread()`检查调用线程是否合法
- 使用线程局部变量确保每个线程只有一个`EventLoop`实例
- 当需要在其他线程的`EventLoop`中执行任务时，通过`runInLoop()`和`queueInLoop()`方法

### LFU缓存

用 2 个哈希表再加上 N 个双链表才能实现先按照频数再按照时间两个纬度的淘汰策略
哈希表 1：hashNode，节点哈希表，用于快速获取数据中 key 对应的节点信息
哈希表 2：hashFreq，频数双向链表哈希表，为每个访问频数 freq 构造一个双向链表 freqList，并且用哈希表联系二者关系以快速定位。

记录当前缓存中的最小频率。

节点中保存前驱指针和后驱指针以及频率，双向链表结构中保存两个哨兵节点（L，R）。这样通过hashNode找到一个节点后，可以迅速通过其频率找到其所在的链表，并对该节点进行更新。

LFU优化，定期将所有项的频率除以某个系数（50%），模拟“访问热度随时间降低”，避免长期驻留的项频率无限累加。

### 堆的复杂度,为什么定时器使用小根堆

- 插入元素O(logn)
- 删除堆顶元素O(logn)
- 获取堆顶元素O(1)
- 查找元素O(n)

对比其他：

- 无序链表 添加O(1)， 查找最小元素需O(n)
- 红黑树，插入 / 删除 / 查找均为 O (log n)，但获取最小元素需 O (log n)

### epoll为什么优于poll、select

1. **避免重复拷贝 FD 集合**
   - select/poll 每次调用都需将 FD 集合从用户空间拷贝到内核空间
   - epoll 通过`epoll_ctl`预先在内核红黑树中注册 FD，后续`epoll_wait`无需重复拷贝
2. **“就绪过滤” 机制**
   - select/poll 的内核实现需要遍历所有监控的 FD
   - epoll 内核维护一个就绪事件链表，`epoll_wait`直接返回该链表

select：FD 数量受限

- 将 FD 集合从用户空间拷贝到内核空间（O (n) 时间）。
- 内核遍历所有 FD，检查就绪状态（O (n) 时间）。
- 返回就绪 FD 的数量，但不告知具体是哪些 FD

poll：使用`pollfd`数组存储 FD，无固定数量限制

- 同样需将 FD 数组从用户空间拷贝到内核空间（O (n) 时间）。
- 内核遍历所有 FD 检查就绪状态（O (n) 时间）。
- 返回就绪 FD 数量，用户空间需遍历数组判断就绪 FD。

epoll：FD 数量无实质限制

- 增删改 FD 时，通过红黑树高效操作（O (log n) 时间）。
- `epoll_wait`：直接返回就绪 FD 链表，无需遍历所有 FD

### 水平触发与边缘触发

- 水平触发：当文件描述符对应的 IO 缓冲区中存在未读取的数据（读事件）或缓冲区未满（写事件）时，会持续触发事件通知。支持阻塞 / 非阻塞 IO
- 边缘触发：当文件描述符的 IO 状态发生变化时（如从不可读变为可读），仅触发一次事件通知，后续状态持续就绪时不再触发。必须使用非阻塞 IO
- 边缘触发一定要非阻塞IO：
  - 假设在 ET 模式下使用阻塞 IO
  - 若数据未读完，阻塞 IO 会等待后续数据，导致线程卡住；
  - 但由于 ET 模式不会再次触发事件，线程将永久阻塞，导致程序卡死

- 阻塞IO：当进程发起 IO 操作时，如果数据未就绪，进程会被挂起（进入等待状态），直到数据就绪并完成操作后才继续执行。阻塞 IO 会导致当前线程暂停执行，无法处理其他任务，直到 IO 完成
- 非阻塞IO：当进程发起 IO 操作时，如果数据未就绪，系统调用会立即返回（通常返回`EWOULDBLOCK`或`EAGAIN`错误），进程可以继续执行其他任务。进程需主动调用`read()`/`write()`尝试操作，若返回错误则继续执行其他任务，稍后再次尝试。

- 

### protobuf

Protocol Buffers是 Google 开发的一种高效、跨平台、语言无关的数据序列化协议，用于结构化数据的编解码。它通过预定义的消息格式（`.proto` 文件）生成代码，广泛应用于 RPC 框架、数据存储、配置文件等场景。

Json与protobuf的对比：

- protobuf将数据编码成二进制格式，Json则是文本格式
- Json可直接读
- protobuf数据体积小很多
- 都是跨语言支持的

protobuf与json的性能差异原因：

- 数据格式的差异，json基于文本，解析时需要逐字符扫描，protobuf的二进制编码直接按字节流处理
- json是动态结构，每次解析需要实时推断类型，protobuf通过.proto文件预定义类型
- protobuf支持零拷贝解析，直接将字节流按照预定义的结构填充到内存中。

### 项目性能测试

raft项目：

- 并发模拟多个 Clerk 向 KVServer 发送大量 RPC 请求（Put/Get），并记录每个请求的延迟、成功/失败数量，以及最终统计出系统的吞吐量与延迟分布
- 创建多个线程，在一段时间内重复发送get或put请求，最终结果为平均每个请求的延迟为40ms左右。

webServer:

- 使用一款性能测试工具webbench，模拟1000个客户端同时对服务器进行60秒的并发请求，在我的电脑配置上最高QPS（每秒多少请求）可达到3万多。



### 内存泄漏检查工具

- 运行时检测
  - Valgrind -- 检测 内存泄漏、越界访问、使用已释放内存
  - AddressSanitizer -- 内存泄漏、越界访问、使用已释放内存、野指针
- 编译器检测
  - Cppcheck -- 内存泄漏、数组越界、未初始化变量
  - Clang-Tidy  --  未释放的内存、空指针解引用、双重释放  可集成到 IDE
- 原理
  - 编译器检测 -- 通过分析源代码，基于规则推断可能的内存泄漏。
  - 在程序运行时监控内存分配和释放操作，记录状态并检测异常。
    - 在`malloc`/`new`被调用时，插入记录；在`free`/`delete`被调用时，标记记录为已释放
    - 在程序结束时，遍历所有分配记录，未标记为释放的记录即为泄漏

### 跳表

- 跳表在**链表的基础上增加了多层索引**，每一层都是一个有序链表的子集。最底层（Level 0）包含所有节点，而更高层的节点通过 “跳跃” 某些节点来加速查找：
- 跳表使用**随机化机制**决定每个节点的层级：插入节点时，每个节点的层级独立随机生成，不受其他节点影响
- 跳表的查找过程从顶层索引开始，逐层向下搜索：从顶层最左侧节点开始，沿当前层向右查找，直到找到第一个**大于等于目标值**的节点或到达链表末尾。如果当前节点的值等于目标值，则查找成功；否则，下降一层继续搜索。

- 

### 一致性哈希

一致性哈希是一种特殊的哈希算法，在分布式系统中广泛应用。它解决了传统哈希在动态节点环境下的数据迁移和负载均衡问题。

- 传统哈希：当节点数量变化（新增或删除节点）时，几乎所有数据的映射关系都会改变，导致大量数据迁移。
- 一致性哈希：
  - **新增节点**：仅影响新增节点逆时针方向到前一个节点之间的数据。
  - **删除节点**：仅影响被删除节点逆时针方向到前一个节点之间的数据，这些数据将迁移到下一个节点

实现：

- 将整个哈希空间（通常是 0~2^32-1）组织成一个虚拟环。
- 将服务器节点（如 IP 地址）哈希到环上的某个位置
- 将数据（如键值对）哈希到环上，顺时针找到第一个节点作为存储位置。

优势：

- 当节点数量变化时，仅需迁移环上相邻节点间的数据，迁移量与总数据量呈线性关系（而非传统哈希的 O (N)）。
- 负载均衡 --  通过虚拟节点（Virtual Nodes）技术，将一个物理节点映射为多个虚拟节点，均匀分布在环上，避免数据倾斜

## -------------云服务和分布式----------------

1. Kubernetes (K8s)中的 Pod, Deployment, Service分别是什么? 
2. 什么是容器?什么是镜像?Docker的基本工作原理是什么? 
3. 微服务架构中，服务发现是如何实现的? 
4. 什么是熔断、降级和限流?它们分别解决了什么问题? 
5. 消息队列 (如Kafka)在系统中通常扮演什么角色?它的“分区"和“消费者组”是什么概念? 
6. 缓存的穿透、击穿、雪崩分别是什么?如何应对？
