## ----Go----

### Goroutine与线程的区别

1. 内存消耗

   线程： 创建一个系统级线程通常需要 1MB - 2MB 的栈内存。
   Goroutine： 初始栈空间仅需 2KB 左右。它会根据需要动态地伸缩（最大可达 1GB）

2. 调度方式 (M:N 模型)
    线程： 由操作系统内核调度。当线程切换时，CPU 需要保存寄存器、更新堆栈指针等，这种“上下文切换”开销较大。Goroutine： 由 Go 运行时 (Runtime) 自行调度。Go 内部有一个名为 G-M-P 的调度模型。

3. 创建与销毁
    线程： 涉及系统调用，开销高。
    Goroutine： 纯用户态操作，创建和销毁的速度极快，几乎可以忽略不计。

### GMP 调度模型

它负责将成千上万个 Goroutine (G) 映射到有限的操作系统线程 (M) 上执行。
其核心组件如下：

- G (Goroutine)： 即协程。它保存了任务执行的上下文（栈、程序计数器 PC 等）。G 只是一个管理任务的数据结构，不直接执行。
- M (Machine)： 操作系统线程。由内核调度，是真正的执行计算资源。M 想要执行 G，必须先绑定一个 P。
- P (Processor)： 逻辑处理器（上下文）。P 维护了一个本地局部队列，里面存放着待运行的 G。
  - P 的数量决定了系统并行的上限（通常等于 CPU 核心数，可通过 GOMAXPROCS 设置）。
  - 核心作用：解耦 G 和 M。即使某个 M 因为系统调用阻塞了，P 可以带着剩下的 G 去找别的 M 执行。

调度策略的核心：

- Work Stealing：当一个 P 的本地队列空了，它会去别的 P 那里“偷”一半的 G 过来执行，防止 CPU 闲置。
- Hand Off：当 M 因为执行 G 发生了阻塞的系统调用时，M 会释放 P，把 P 转交给其他的 M（或创建新 M）去接管，保证 P 里的其他 G 不被阻塞。

###  Goroutine 发生抢占的情况

1. 基于协作的抢占（在函数调用点）

   Go 编译器会在函数的入口处插入一段名为 morestack() 的检测代码。

   - 触发：运行时如果发现某个 G 执行时间过长（超过 10ms），会给该 G 发送一个“抢占标记”。

   - 发生：当该 G 执行到下一个函数调用时，会触发堆栈检测，发现标记后主动让出 CPU 权限，挂起到就绪队列。

2. 基于信号的真抢占（非协作式，v1.14+）

   为了处理那种“连函数都不调用”的纯计算死循环，Go 引入了基于信号的抢占。

   - 触发：监控线程（sysmon）发现某个 G 运行超过 10ms。

   - 发生：内核向执行该 G 的线程（M）发送一个 SIGURG 信号。
   - 后果：M 收到信号后会被硬件中断，进而进入信号处理函数。Go 运行时会在处理函数中强制挂起当前 G，并调度下一个 G。这解决了死循环导致的程序卡死问题。

3. 系统调用（Syscall）抢占
   场景：当 G 发起一个同步系统调用（如读取大文件）。
   过程：监控线程（sysmon）会发现 P 处于 Syscall 状态。它会将 P 与当前的 M 解绑（P 重新寻找空闲的 M 执行后续 G），而旧的 M 则带着 G 慢慢等系统调用结束。这本质上也是一种对 P 资源的抢占和重分配。

### goroutine泄露

​	是指你启动了一个 Goroutine，但由于某种原因（如逻辑阻塞或死循环），它**永远无法结束**，也无法被垃圾回收（GC）销毁。

场景：

1. Channel 导致的阻塞
   	发送者阻塞：向一个没有接收者的无缓冲 Channel 发送数据。
   	接收者阻塞：从一个永远不会有数据存入、也永远不会关闭的 Channel 接收数据。

2. Select 语句没有 Default 分支

3. Goroutine 试图获取一个永远不会被释放的锁。

4. 循环中的 Goroutine 未退出

   ​	例如，在一个 `for` 循环中启动了监听任务，但没有设置退出机制（如 Context 取消）。

### 逃逸分析

作用：确定一个变量应该分配在栈上还是堆上。
为什么需要逃逸分析？
	栈（Stack）： 分配和回收极快（仅需移动栈指针），随着函数返回自动释放，压力小。
	堆（Heap）： 需要垃圾回收（GC）来清理。如果所有变量都放堆上，GC 压力会巨大，导致程序变慢。

什么情况会逃逸？
 	1. 函数返回局部变量的指针： 如果函数返回一个局部变量的地址，该变量在函数结束后必须继续存在，因此必须分配在堆上
 	2. 接口（interface{}）类型存储
 	3. 闭包（Closure）引用：闭包函数如果引用了外部函数的局部变量，为了保证闭包在外部函数退出后仍能正常运行，这些变量会逃逸到堆。
 	4. 变量空间过大或由于动态大小：在 `make` 切片时，如果长度是**变量**而不是常量，编译器无法在编译期确定其大小，通常也会逃逸。

​	https://dablelv.github.io/go-coding-advice/%E7%AC%AC%E5%9B%9B%E7%AF%87%EF%BC%9A%E6%9C%80%E4%BD%B3%E6%80%A7%E8%83%BD/2.%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/3.%E5%87%8F%E5%B0%91%E9%80%83%E9%80%B8%EF%BC%8C%E5%B0%86%E5%8F%98%E9%87%8F%E9%99%90%E5%88%B6%E5%9C%A8%E6%A0%88%E4%B8%8A.html

如何查看逃逸分析结果？
	在执行 `go build` 或 `go run` 时，加上 `-gcflags="-m"` 参数。

### 垃圾回收机制

1. 三色标记法
   为了实现并发扫描（即 GC 和用户程序同时运行），Go 将对象分为三种颜色：
   	白色：潜在的垃圾，其内存可能会被回收。
   	灰色：活跃对象，但其引用的对象尚未被扫描。
   	黑色：活跃对象，且其引用的对象已全部扫描完成。

​	GC 过程：
​		起初所有对象都是白色。
​		从根对象（栈、全局变量）开始，将其标记为灰色。
​		遍历灰色对象，将它们引用的对象转为灰色，自身转为黑色。
​		重复此过程，直到没有灰色对象。
​		清理剩下的白色对象。

2. 混合写屏障 (Hybrid Write Barrier)

   这是 Go 1.8 引入的黑科技，它解决了在并发标记期间，用户程序修改指针导致对象丢失的问题。

减少GC停顿时间的优化思路
	本质是**减少堆内存的分配压力**和**减少扫描对象数量**。

- 减少内存分配（复用对象）
- 利用逃逸分析（让变量留在栈上）
- 避免频繁创建小对象

### error 和 panic

`error` 是一个接口。它是**业务逻辑的一部分**，代表“可能会发生且需要处理的情况”。
`panic` 类似于其他语言中的 `Unchecked Exception`。它代表程序进入了**不可恢复的非法状态**。

### 接口和泛型

1. 接口 (Interface)：解决的是“行为抽象”问题，多个类型有共同的操作
    接口定义了一组行为（方法）。它不关心对象是什么，只关心对象能做什么。

2. 泛型 (Generics)：解决的是“类型安全与代码复用”问题，逻辑相同但类型不同
泛型允许你在编写函数或数据结构时使用类型占位符，直到使用时才确定具体类型。

3. interface{} 和 any 的关系
any是别名

4. 使用泛型相比接口的优势
    虽然泛型和接口在某些场景下可以互换，但泛型有以下显著优势：

    ​	1. 类型安全（无需断言）
    
       	接口：如果你把数据存入 []interface{}，取出时必须进行类型断言 v.(int)，这在运行时可能触发 panic。
       	泛型：在编译阶段就确定了类型。如果你定义了 List[T]，当你创建 List[int] 时，编译器会确保你只能存入 int，取出时直接就是 int。
    
    ​	2. 更好的性能
    
       	接口：涉及 动态派发，通常需要通过 itab 查找方法表，且往往会导致变量逃逸到堆上，增加 GC 压力。
       	泛型：Go 编译器会对泛型进行 “实例化”。在编译时生成特定类型的版本（或类似机制）。它更接近于静态调用，对编译器优化更友好，减少了运行时的装箱/拆箱开销。
    
    ​	3. 某些功能实现更加复杂
    
       	例子：写一个 Max(a, b T) T 函数。
    
       ​	用接口很难实现，因为你需要定义一个 Comparable 接口，而且返回值往往需要类型断言回具体类型。
       ​	用泛型非常简单：`func Max[T constraints.Ordered](a, b T) T { ... }`。它能保证输入和输出的类型是完全一致的。

1. 

### context的应用场景

​	context包是管理并发控制、生命周期和跨层级传递数据的标准方式。它主要解决了 “如何安全、高效地通知成千上万个 Goroutine 停止工作” 的问题。

1. 超时控制 (Timeout)
   在访问数据库、调用第三方 API 或执行耗时操作时，我们不能让请求无限期等待。

   ​	用法：context.WithTimeout

2. 取消信号传递 (Cancellation)
   当一个父级请求因为某种原因（如用户关闭了浏览器）被取消时，该请求派生出的所有下游 Goroutine 都应该立即停止执行，以节省资源。

   ​	场景：主请求取消后，停止相关的数据库查询和缓存写入。

   ​	用法：context.WithCancel

3. 截止时间控制 (Deadline)
    与超时类似，但它的是一个具体的时间点（Time object），而不是时间段（Duration）。

  ​	用法：context.WithDeadline

4. 跨层级元数据传递 (Value)
    在请求的生命周期内，在不同的函数、中间件或服务层之间传递一些公共信息。

  ​	场景：传递 TraceID（链路追踪）、UserID（当前登录用户）、Token。

  ​	用法：context.WithValue

  ​	注意：不要用它传递业务参数。它应该是与业务逻辑无关、用于横向关注点（如日志、监控）的数据。

注意事项：

1. 不要将 Context 存储在结构体中：Context 应该作为函数的第一个参数显式传递，变量名通常叫 ctx。
2. 不要传递 nil Context：如果你不确定要传什么，使用 context.TODO()，而不是 nil。
3. Context 是不可变的（Immutable）：每次衍生（WithCancel, WithValue 等）都会返回一个新的子 Context。当**父级被取消时，所有子级都会收到信号**。
4. 及时调用 Cancel（除了WithValue之外的创建的方式会返回Cancel函数）：无论是超时还是正常结束，只要使用了派生的 Context，一定要记得 defer cancel()，否则会导致 Goroutine 泄露或计时器无法释放。

### go的内存模型

​	Go 的内存模型规定了：**“在一个 Goroutine 中对变量进行写入，在什么条件下能保证被另一个 Goroutine 观察到。”**

#### Happens Before

​	Happens-Before 是一种的“可见性保证”。如果事件 E_1 Happens-Before 事件 E_2，那么 E_2 的执行者一定能看到 E_1 产生的所有结果（比如对变量的赋值）。只有符合规则才能**确定代码执行顺序**

在 Go 中，以下是一些典型的 Happens-Before 规则：

1. 单协程规则：在同一个 Goroutine 中，代码逻辑上的顺序就是 Happens-Before 顺序。
2. Channel 规则（核心）：
   1. 发送 Happens-Before 接收完成。
   2. Channel 关闭 Happens-Before 接收到零值。
   3. 对于无缓冲 Channel，接收完成 Happens-Before 发送完成（强制同步）。
3. 锁规则：第 n 次 Unlock Happens-Before 第 n+1 次 Lock。
4. 启动规则：go 语句的执行 Happens-Before 该 Goroutine 内部的第一行代码。

#### Don't communicate by sharing memory; share memory by communicating

传统方式：通过共享内存来通信 (Share memory to communicate)

- 做法：两个线程通过读写同一个变量（如全局变量、Map）来交换信息。
- 代价：为了防止竞争，必须频繁加锁（Mutex）。
- 痛点：锁竞争（Lock Contention）会导致性能下降，且容易产生死锁（Deadlock）或竞态条件（Race Condition）。

Go 的方式：通过通信来共享内存 (Share memory by communicating)

- 做法：通过 Channel 发送数据。当一个 Goroutine 将数据的指针发给另一个协程后，它就不再操作这个指针了。
- 核心逻辑：数据的**所有权发生了转移。**
- 优势：
  - 天然同步：Channel 内部保证了 Happens-Before。
  - 降低心智负担：不需要在业务逻辑里到处考虑 Lock/Unlock。
  - 解耦：生产者和消费者不需要知道对方的存在，只需要关注 Channel。

### slice实现

```go
type slice struct {
    array unsafe.Pointer // 指向底层数组起始地址的指针
    len   int            // 切片的长度（当前存储的元素个数）
    cap   int            // 切片的容量（从指针处开始，底层数组的大小）
}
```

扩容机制（以 Go 1.18+ 为准）
当你使用 append 且容量不足时，Go 会触发扩容逻辑：

- 小容量：如果期望容量小于 256，则直接翻倍（2x）。
- 大容量：如果期望容量超过 256，则不再简单翻倍，而是按照公式 newcap += (newcap + 3*256) / 4 逐渐增长。
- 内存对齐：计算完大致容量后，Go 还会根据内存分配器的规格进行向上取整（内存对齐），所以最终分配的 cap 往往比计算出的稍微大一点。

### map实现

​	 Map 使用了**哈希表实现，采用链地址法**（通过 Bucket 桶）来解决哈希冲突。

```go
type hmap struct {
    count     int    // 元素个数
    B         uint8  // 桶的数量，数量为 2^B
    buckets   unsafe.Pointer // 指向数组（桶）的指针
    oldbuckets unsafe.Pointer // 扩容时指向旧桶的指针（用于渐进式搬迁）
    extra     *mapextra      // 溢出桶相关
}
```

1. 桶的内部 (bmap)
   每个桶（Bucket）最多存储 8 个键值对。
   - 为了内存对齐，桶内部先存储 8 个 Key，再存储 8 个 Value。
   - 桶内还有一个 tophash 数组（8 个字节），记录了每个 Key 哈希值的高 8 位，用于快速定位 Key。
   - 如果一个桶存满了（8 个全满），会通过 overflow 指针指向一个溢出桶。

2. 扩容逻辑
   Map 的扩容是渐进式的（每次进行写或删操作时搬迁 1-2 个桶），避免瞬间性能抖动。
   - 翻倍扩容：当装载因子（Load Factor）超过 6.5 时（即元素太多），桶的数量翻倍。
   - 等量扩容：当溢出桶过多但元素不多时，会进行整理，让数据更紧凑。

### nil与空值的区别

1. 什么是空值 (Zero Value)？
    在 Go 中，当你声明一个变量但不初始化它时，Go 编译器会自动赋予该变量一个默认值，这就是 空值。

  每种基础类型都有自己的空值：

  - 数值类型 (int, float, etc.): 0
  - 布尔类型 (bool): false
  - 字符串类型 (string): "" (空字符串)
  - 复数类型: 0+0i

  重点： 对于上述基础类型，它们永远不会是 nil。

2. 什么是 nil？
    nil 是 Go 中的一个预定义标识符，代表指针、接口、切片、映射、通道和函数的“零值”。

  它表示该变量目前没有指向任何底层的内存地址或数据结构。

  可以为 nil 的类型：

  - 指针 (*int, *Struct)
  - 切片 ([]int)
  - 映射 (map[string]int)
  - 通道 (chan int)
  - 接口 (interface{} / any)
  - 函数 (func())

​	一个接口变量包含两个部分：Type（类型）和 Value（值）。 只有当 Type 和 Value **同时为 nil** 时，接口才等于 nil。

### 工具-trace, pprof, benchmark

### 函数是值传递还是引用传递

​	所有的函数传参都是值传递
​	切片作为参数传入函数，进行append，外部是看不到变化的，因为外部的len没有变化（外部打印切片会根据len长度打印），对已存在的元素值进行修改，外部可以看到，因为底层数组是指针形式。

### 缓冲、无缓冲 chan的区别

1. 无缓冲 Channel (make(chan T))
    无缓冲通道也被称为同步通道。
    特性：它的容量为 0，不能存储任何数据。
    行为：

  - 发送者会阻塞，直到有接收者准备好取走数据。
  - 接收者会阻塞，直到有发送者准备好发送数据。

  同步性：它保证了发送和接收是同时发生的（Handshaking）。这种机制确保了两个 Goroutine 在数据交换的那一刻处于同步状态。

2. 有缓冲 Channel (make(chan T, capacity))
    有缓冲通道类似于一个固定大小的队列。
    特性：拥有一个内部队列，可以暂存 capacity 个元素。
    行为：

  - 发送者：只要缓冲区没满，发送操作就是非阻塞的，数据存入缓冲区后立即返回；只有当缓冲区满时，发送者才会阻塞。
  - 接收者：只要缓冲区有数据，接收操作就是非阻塞的；只有当缓冲区空时，接收者才会阻塞。

  同步性：它是异步的。发送者和接收者之间存在解耦。

### init的执行顺序

1. 跨包之间的顺序
   如果 main 包导入了 A，A 导入了 B：
   执行方向：B -> A -> main。
   逻辑：只有**被依赖**的包完全初始化好了，依赖它的包才能开始初始化。

2. 单个包内部的顺序
   在一个包（package）内部，初始化的优先级如下：
   常量 (const)
   变量 (var)
   init() 函数

3. 同一个包下的多个文件
   如果一个包下有多个 .go 文件，每个文件都可以包含一个或多个 init 函数。
   执行顺序：按照文件名在编译器中的 字符串排序 (Lexical Order) 顺序执行。例如 a.go 里的 init 会先于 b.go 执行。

4. 同一个文件下的多个 init
   在同一个源文件中，init 函数会按照它们在代码中出现的 先后顺序 依次执行。

### 子协程panic为何没法被父协程recover

​	每个 Goroutine 都有自己独立的 调用栈（Stack）。recover 的机制是：它只能捕获 当前 Goroutine 栈上的 panic。

​	当子协程发生 panic 时，Go 运行时会开始“栈回溯（Stack Unwinding）”，试图在当前协程的栈中寻找 defer 函数里的 recover。如果回溯到栈顶还没找到，程序就会认为发生了不可恢复的错误，从而直接终止整个进程。

父协程此时正在自己的栈上运行，它完全感知不到另一个协程栈上的崩溃信号。

### Finalizer的作用

Finalizer（析构器）是一种特殊的机制，允许关联一个函数到一个对象上。当垃圾回收器（GC）发现该对象已经不可达，准备回收其内存之前，会先调用这个关联的函数。用于确保那些忘记手动关闭的资源最终能被释放。不建议使用。

### go的对象池方案及解决的问题

​	sync.Pool，减轻垃圾回收（GC）的压力。

## ----计算机网络----

### 三次握手

- 客户端发送SYN报文，初始化序列号，进入SYN_SEND
- 服务端发送 SYN+ACK报文，初始自己的序列号，进入SYN_RECV
- 客户端发送ACK报文，客户端进入ESTABLISHED，服务端收到这个ACK后，也进入ESTABLISHED
- 如果没有第三次，服务器不能确定客户端能否正确收到。

### 四次挥手

- 客户端发送FIN报文，进入FIN-WAIT-1
- 服务端回复ACK，服务端进入CLOSE-WAIT，客户端收到后进入FIN_WAIT-2
- 服务端可继续发送数据
- 服务端发送FIN，进入LAST-ACK
- 客户端回复ACK，客户端进入TIME-WAIT，服务端收到后进入CLOSE

- 为什么要四次？
  - TCP 是全双工协议（两边都能说话），需分别关闭两个方向的连接。
  - 被动关闭方可能需要时间处理剩余数据，不能立即发送 `FIN`。

- 为什么客户端要等待2MSL？MSL - 报文最大生存时间
  - 确保最后一个ACK报文能够到达服务器，ACK如果丢失，服务器收不到ACK，超时重传，客户端收到后重新发送
  - 确保在本连接持续期间所产生的所有报文段都从网络中消失，从而避免新旧连接之间的干扰‌

### 流量控制

- 防止发送方**发送速率过快**导致接收方缓冲区溢出。
- **核心机制**：滑动窗口 -  接收方通过 TCP 报文段中的**窗口字段（Window Size）** 告知发送方自己当前的接收缓冲区可用空间，发送方据此限制发送数据的大小。

### 拥塞控制

避免网络因**过量数据**导致拥堵，通过动态调整发送方的**拥塞窗口**平衡全局负载， 

- 慢启动 -- 收到一个ACK，拥塞窗口就指数增长，超过阈值，进入下一阶段
- 拥塞避免--线性增长，
- 快速重传--收到三个重复ACK，立即重传丢失的报文，无需等待超时。
- 快速恢复 -- 重传后，将阈值设为当前拥塞窗口 的一半。拥塞窗口设为阈值，并进入拥塞避免阶段

### TCP与UDP对比

- TCP面向连接，UDP无连接
- TCP可靠传输，通过握手、确认、重传、拥塞控制机制。UDP不可靠传输、不保证数据不丢失
- TCP有状态、会记录消息是否发送、是否被接收等、UDP无状态
- TCP面向字节流、UDP面向报文
- TCP传输效率低
- TCP首部开销 20~60字节，比UDP首部 8 字节大
- TCP只支持点对点通信，UDP支持广播或多播
- 使用场景
  - TCP：要求通信数据可靠场景 -- 网页浏览、文件传输、邮件传输
  - UDP：要求通信速度⾼场景  --  域名转换、视频直播、实时游戏

### TCP确保可靠

- 为每个数据包指定序号
- 校验和，校验首部和数据部分，接收端计算检验和，若改变了，则丢弃
- 流量控制：TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据
- 拥塞控制
- 确认应答
- 超时重传

### KeepAlive

- TCP的Keepalive：⼀种⽤于在 TCP 连接上检测空闲连接状态的机制，就是TCP有⼀个定时任务做倒计时，超时后会触发任务，内容是发送⼀个探测报⽂给对端，⽤来判断对端是否存活。  
- HTTP的Keep-Alive：用于设置开启长连接

### 强缓存和协商缓存

- 强缓存：服务端返回资源时带上一个过期时间字段，当客户端再次访问资源，浏览器判断是否已经过期，未过期可直接读取缓存
- 协商缓存：客户端第一次访问资源，服务端会根据该文件生成一个表示文件是否改动了的标志（last-modified返回文件修改时间，`ETag`返回文件哈希值），客户端将资源缓存，后续客户端访问该资源时，会先发送该标志给服务端，服务器判断资源是否修改，未修改客户端可直接读取缓存。

### HTTP迭代

- HTTP 0.9 ：只支持GET方法，没有请求头
- HTTP 1.0 ：引入请求头和响应头，只能短连接，每次请求都要建立一个TCP连接
- HTTP 1.1 ： 支持长连接，管道化使得请求能够并行发送（也就是发送请求后，不必收到回应就继续发送请求），Host字段在同一个`IP`地址上承载多个域名
- HTTP 2 ：基于HTTPS，将数据分割为二进制帧进行传输，多路复用 -- 同一个HTTP连接发起多个请求-响应，首部压缩（`HPACK`算法），服务器可以自主向客户端推送资源
- HTTP 3 ：基于 `QUIC` 协议构建，使用 `UDP` 作为传输层协议，HTTP/3 继承并优化了 HTTP/2 的多路复用特性

### 如何建立HTTPS连接 

1. 客户端发送请求
2. 服务端生成一对公私钥，公钥传给CA机构，CA机构使用自己的私钥对服务器公钥加密，生成数字证书
3. 服务器将数字证书传回客户端
4. 客户端浏览器解析数字证书，得到服务端的公钥
5. 客户端随机生成一个密钥，用服务器公钥加密，传给服务器
6. 服务器使用自己的私钥解密，得到客户端的密钥
7. 两端使用这个密钥进行加密解密通信

### GET和POST请求的区别  

- GET⽤于从服务端获取资源， POST用来向服务器端提交数据
- GET请求的参数一般写在URL中，POST请求参数放在请求体
- 安全性不同
- GET 请求会被浏览器主动cache，⽽ POST 不会，除⾮⼿动设置。  

### 在浏览器中输入URL

结合网络模型：

1. 浏览器根据 URL 确定要访问的资源，并构建 HTTP 请求报文
2. 浏览器根据URL解析出域名，并检查缓存中是否有对应IP地址，没有则向 DNS 服务器发送域名解析请求，将域名转换为 IP 地址。
3. TCP开始三次握手，将应用层的 HTTP 请求报文封装成 TCP 报文段，添加源端口和目的端口等信息。如果是 HTTP/3 版本，会使用 UDP 协议并结合 QUIC 等机制来提高性能。
4. 将 TCP 报文段（或 UDP 数据报）封装成 IP 数据包，添加源 IP 地址和目的 IP 地址。
5. 构建数据包时，已知目标 IP 地址。通过查找本地路由表，判断目标 IP 是否在本地子网。若不在，选择默认网关作为中间跳点。
6. 将 IP 数据包封装成帧，添加源 MAC 地址和目的 MAC 地址等链路层头部信息，根据ARP协议，通过下一跳IP地址得到MAC地址。

不结合分层模型：

1. 浏览器会解析出协议、主机、端⼝、路径等信息，并构造⼀个HTTP请求  
2. DNS域名解析, 将域名解析成对应的IP地址  
3. 建⽴起TCP连接之三次握⼿  
4. 浏览器发送HTTP/HTTPS请求到web服务器  
5. 服务器处理HTTP请求并返回HTTP报⽂  
6. 浏览器渲染⻚⾯  
7. 断开连接之TCP四次挥⼿ 

### DNS查询过程 -- 客户端向本地域名服务器发起递归查询，其余过程都是迭代查询

1.  浏览器缓存
2.  本地的host文件
3.  向本地DNS服务器发送查询请求
4.  本地DNS服务器向根域名服务器查询，根域名服务器不进行解析，而是告知向哪个顶级域名服务器继续查询
5.  本地DNS服务器继续向顶级域名服务器查询，后者告知向哪个权威域名服务器查询
6.  本地DNS服务器向该权威域名服务器继续查询，得到对应IP地址。

### CDN

​	全称为内容分发⽹络 （Content Delivery Network） , 通过将内容存储在分布式的服务器上，使⽤户可以从距离较近的服务器获取所需的内容，从⽽减少数据传输的时间和距离，提⾼内容的传输速度、减少延迟和提升⽤户体验。  

### SYN攻击

​	攻击者伪造不同IP地址的SYN报⽂请求连接，服务端收到连接请求后分配资源，回复ACK+SYN包，但是由于IP地址是伪造的，⽆法收到回应，久⽽久之造成服务端半连接队列被占满，⽆法正常⼯作。  

## ----数据结构----

### 二叉搜索树的问题及改进

BST 的致命弱点在于：它不保证树的“平衡”。树的形态严重依赖于数据的 插入顺序。

1. 退化为链表如果插入的数据是已排序的（例如 1, 2, 3, 4, 5），BST 会长成一个只有右子树的“歪脖子树”。后果：此时查找效率从 $O(\log n)$ 直接退化为 $O(n)$。它失去了树的优势，变成了性能最差的单链表。

2. 性能不稳定性由于没有强制平衡机制，BST 的操作性能非常不稳定。在极端情况下，原本毫秒级完成的查询可能会变得极其缓慢。


改进的核心思路是引入 “自平衡机制”。无论插入顺序如何，树都要通过自动调整来保证高度尽可能小（即保持平衡）。

1. AVL 树（高度平衡树）AVL 树是最早的平衡方案。它要求任何节点的左右子树高度差不能超过 1。
2. 红黑树 (Red-Black Tree)红黑树放弃了“绝对平衡”，追求“大致平衡”。它通过给节点染色（红/黑）并遵循 5 个约束规则来实现。

### LFU缓存

用 2 个哈希表再加上 N 个双链表才能实现先按照频数再按照时间两个纬度的淘汰策略
哈希表 1：hashNode，节点哈希表，用于快速获取数据中 key 对应的节点信息
哈希表 2：hashFreq，频数双向链表哈希表，为每个访问频数 freq 构造一个双向链表 freqList，并且用哈希表联系二者关系以快速定位。

记录当前缓存中的最小频率。

节点中保存前驱指针和后驱指针以及频率，双向链表结构中保存两个哨兵节点（L，R）。这样通过hashNode找到一个节点后，可以迅速通过其频率找到其所在的链表，并对该节点进行更新。

LFU优化，定期将所有项的频率除以某个系数（50%），模拟“访问热度随时间降低”，避免长期驻留的项频率无限累加。

### 跳表

- 跳表在**链表的基础上增加了多层索引**，每一层都是一个有序链表的子集。最底层（Level 0）包含所有节点，而更高层的节点通过 “跳跃” 某些节点来加速查找：
- 跳表使用**随机化机制**决定每个节点的层级：插入节点时，每个节点的层级独立随机生成，不受其他节点影响
- 跳表的查找过程从顶层索引开始，逐层向下搜索：从顶层最左侧节点开始，沿当前层向右查找，直到找到第一个**大于等于目标值**的节点或到达链表末尾。如果当前节点的值等于目标值，则查找成功；否则，下降一层继续搜索。

### 一致性哈希

一致性哈希是一种特殊的哈希算法，在分布式系统中广泛应用。它解决了传统哈希在动态节点环境下的数据迁移和负载均衡问题。

- 传统哈希：当节点数量变化（新增或删除节点）时，几乎所有数据的映射关系都会改变，导致大量数据迁移。
- 一致性哈希：
  - **新增节点**：仅影响新增节点逆时针方向到前一个节点之间的数据。
  - **删除节点**：仅影响被删除节点逆时针方向到前一个节点之间的数据，这些数据将迁移到下一个节点

实现：

- 将整个哈希空间（通常是 0~2^32-1）组织成一个虚拟环。
- 将服务器节点（如 IP 地址）哈希到环上的某个位置
- 将数据（如键值对）哈希到环上，顺时针找到第一个节点作为存储位置。

优势：

- 当节点数量变化时，仅需迁移环上相邻节点间的数据，迁移量与总数据量呈线性关系（而非传统哈希的 O (N)）。
- 负载均衡 --  通过虚拟节点（Virtual Nodes）技术，将一个物理节点映射为多个虚拟节点，均匀分布在环上，避免数据倾斜

## ----操作系统----

### 进程通信

- 管道 -- 仅用于父子进程
- 命名管道 -- 允许⽆亲缘关系进程间的通信。  
- 消息队列 -- 消息队列是消息的链表，存放在内核中并由消息队列标识符标识  
- 共享内存  --  就是映射⼀段能被其他进程所访问的内存，这段共享内存由⼀个进程创建，但多个进程都可以访问。
- 信号量
- 信号
- 套接字 -- 主要⽤于在客户端和服务器之间通过⽹络进⾏通信。

### 栈与堆

函数调用是如何使用栈的？

​	函数调用利用了栈**“先进后出”的特性。每当一个函数被调用时，系统会在栈顶为它分配一块内存区域，称为栈帧（Stack Frame）**。栈帧里存了什么？

 	1. 返回地址：函数执行完后，下一行代码该回哪里执行。
 	2. 函数参数：传入函数的变量值。
 	3. 局部变量：函数内部定义的非逃逸变量。
 	4. 寄存器状态：保存调用前的 CPU 状态，以便恢复。

为什么堆上的内存分配比栈慢? 主要原因有以下三点：

	1. 分配算法复杂度不同
	- 栈：只需要移动一下 栈指针即可。这在 CPU 指令集里只是一个简单的减法运算，时间复杂度为 O(1)。
	- 堆：操作系统必须在维护的空闲内存链表或位图中寻找一块足够大的空间。这涉及到遍历、搜索算法（如 First-fit 或 Best-fit），甚至需要合并相邻的空闲块，复杂度远高于栈。
	2. 系统调用开销
	- 栈：分配是在用户态直接完成的。
	- 堆：如果堆内存不足，程序需要向操作系统发出系统调用（如 Linux 的 brk 或 mmap）来申请更多的内存页。系统调用涉及用户态与内核态的上下文切换，非常耗时。
	3. 碎片处理与内存对齐
	- 栈：内存是连续排列的，永远不会有碎片。
	- 堆：频繁的申请和释放会导致内存块变得支离破碎。为了找到合适的块，分配器可能需要做更多计算，甚至触发内存整理。

### 进程、线程、协程

- 进程：操作系统**资源分配的基本单位**，包含独立的地址空间、代码段、数据段和系统资源（如文件句柄、内存空间）
- 线程：进程内的执行单元，共享进程的地址空间和系统资源（如内存、文件句柄）但拥有自己**独立的栈**。线程是 CPU **调度的基本单位**，同一进程内的线程可并发执行，减少了进程间切换的开销（进程切换需要切换地址空间、页表等操作、开销大）。
- 协程：又称 “用户态线程”，是由用户空间管理的轻量级执行单元。适合于高并发IO场景。

协程相比于线程的优势：

- 协程在用户态调度，开销小
- 协程的内存占用极小
- 线程需要操作系统内核维护线程控制块、调度队列等资源，而协程不占用内核资源。

什么场景下选择多线程？
多线程主要利用多核 CPU 的并行计算能力，适用于 CPU 密集型 任务：

- 大规模数值计算：如视频转码、科学计算、图形渲染。
- 并行处理：当任务需要独占 CPU 核心以获取最大吞吐量时。

什么场景下考虑使用协程？
协程的优势在于处理海量的并发连接，特别适用于 I/O 密集型 任务：

- 高并发 Web 服务：如处理成千上万个并发的 HTTP 请求或 WebSocket 连接。
- 网络爬虫：需要同时等待大量网络响应。
- 微服务通信：频繁进行 RPC 调用，大部分时间在等待网络 I/O，使用协程可以极大地节省线程切换带来的开销。

### I/O模型

阻塞、非阻塞 vs 同步、异步

- 阻塞/非阻塞：关注的是调用者的状态。
  - 阻塞：结果没出来前，我什么都不干，原地等待。
  - 非阻塞：结果没出来前，我先干别的，但我要隔一会儿来问一下（轮询）。

- 同步/异步：关注的是消息通知机制。
  - 同步：我（调用者）得自己去盯着任务进度，或者等任务做完。
  - 异步：任务交给别人（内核）去做，我去做别的，做完了你（内核）直接把结果塞给我并通知我。

五种 I/O 模型对比

- 阻塞 I/O (Blocking I/O)：最简单。调用 recvfrom，内核没收到数据就一直等着。
- 非阻塞 I/O (Non-blocking I/O)：调用后立即返回 EAGAIN，用户进程需要不断轮询内核，直到数据准备好。
- I/O 多路复用 (I/O Multiplexing)：如 select/epoll。进程阻塞在 select 调用上，同时监听多个 Socket。
- 信号驱动 I/O (Signal Driven I/O)：数据准备好时内核发信号给进程。
- 异步 I/O (Asynchronous I/O)：如 Linux 的 AIO 或 Windows 的 IOCP。真正的异步：数据拷贝进用户空间后才通知你。

### Reactor模式

 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**

- 单Reactor单线程
  - Reactor对象通过Select监听事件，如果是连接建立事件，分发给Acceptor对象进行处理，如果是其他事件，交给handler对象进行处理
- 单Reactor多线程
  - reactor对象监听事件，根据事件类型发送给Acceptor对象或者handler对象
  - handler对象会将raad读取到的数据交给子线程进行处理，处理完后子线程会返回结果给handler，handler完成对client的响应。
- 多Reactor多线程
  - 主线程的MainReactor监听**连接建立事件**, 收到事件后通过Acceptor获取连接，将连接分配给某个子线程
  - 子线程的SubReactor将该连接加入IO多路复用进行监听，
  - 该连接有事件发生时，子线程对事件进行处理

### Proactor模式

异步网络模式，感知的是已完成的读写事件

### CPU调度算法

1. 先来先服务

2. 短作业优先 - 长任务饿死

3. 轮转调度

   为每个进程分配一个固定的小时间片。时间片用完后，进程被强行剥夺 CPU 并送回队列末尾。

4. 优先级调度

   给每个进程分配一个优先级，CPU 总是优先执行优先级最高的进程。

5. 多级反馈队列

   - 设置多个队列，每个队列优先级不同。

   - 新任务先进入最高优先级队列（时间片很短）。

   - 如果任务在时间片内没干完，就降级到下一级队列（时间片更长）。

   - 只有高优先级队列为空，才运行低优先级队列。

### CPU缓存一致性

​	在现代多核 CPU 中，每个核心都有自己的 L1、L2 缓存（私有），并共享 L3 缓存和主内存。

​	问题在于： 当两个核心**同时读取了同一个变量**到各自的私有缓存中，其中一个核心修改了该变量，另一个核心缓存里的数据就变成了“过时”的脏数据。 缓存一致性（Cache Coherence） 就是一套协议机制，**确保多个核心看到的内存数据是一致的**，防止程序逻辑出错。

MESI 是最经典的缓存一致性协议，它为缓存行（Cache Line）定义了四种状态：

1. **M (Modified, 已修改)**：该行数据已被修改，且只存在于当前缓存中，与主存数据不一致。
2. **E (Exclusive, 独占)**：该行数据与主存一致，且只存在于当前缓存中。
3. **S (Shared, 共享)**：该行数据与主存一致，且在多个核心的缓存中都存在。
4. **I (Invalid, 无效)**：该行数据已失效，不能使用，必须重新从主存或其它缓存读取。

**核心逻辑：** 当核心 A 想要修改处于 **S** 状态的变量时，它必须先向总线广播一个信号，将核心 B 中对应的缓存行标记为 **I**（无效），然后 A 才能将状态改为 **M** 并进行写入。

### 自旋锁和互斥锁

1. 互斥锁 (Mutex)
   当一个线程尝试获取互斥锁但锁已被占用时，该线程会释放 CPU，进入睡眠（休眠）状态。
   - 行为：线程告诉操作系统：“我先睡会儿，锁空出来了你叫醒我。” 操作系统将其放入等待队列，并把 CPU 分配给其他线程。
   - 开销：涉及两次上下文切换（线程休眠一次，唤醒一次）。上下文切换需要保存寄存器状态、修改页表等，开销较大（通常在微秒级别）。
   - 适用场景：
     - 锁持有的时间较长（比如涉及磁盘 I/O、数据库查询）。
     - 单核 CPU 环境（因为单核下自旋没意义，持有锁的线程没机会运行来释放锁）。

2. 自旋锁 (Spinlock)
    当一个线程尝试获取自旋锁但锁已被占用时，该线程不会睡眠，而是继续占用 CPU，在一个循环里不停地查询锁是否可用。
  - 行为：线程一直在问：“锁好了吗？好了吗？好了吗？” 它始终处于运行状态（Busy-waiting）。
  - 开销：没有上下文切换，但它在等待期间会持续消耗 CPU。
  - 适用场景：
    - 锁持有的时间极短（比如仅仅修改一个计数器）。
    - 多核 CPU 环境（一个核在自旋，另一个核在运行释放锁的逻辑）。
    - 中断处理程序（在 Linux 内核中，中断上下文不能睡眠，必须使用自旋锁）。

### 读写锁的实现

允许多个线程同时读取共享资源（读锁可重入），但在写入时需要独占访问（写锁排他）。这种设计提高了并发性能，适合读多写少的场景。

- **状态跟踪**：通常使用一个计数器记录当前读者数量，以及一个标志位表示是否有写者等待或持有锁。
- **互斥锁**：保护内部状态变量的访问，确保操作的原子性。
- **条件变量**：用于线程间的通知机制，当写锁释放时通知等待的读者 / 写者。
- **读锁获取**：如果没有写者持有锁或等待，允许获取读锁并增加读者计数。
- **读锁释放**：减少读者计数，若减为 0 则通知可能等待的写者。
- **写锁获取**：等待直到没有读者和写者，然后标记写锁被持有。
- **写锁释放**：标记写锁释放，并通知所有等待的读者和写者。

### 内存对齐

内存管理中的一种优化技术，它确保数据在内存中的起始地址是特定值的整数倍

- 结构体的对齐值：取其数据成员中的最大对齐值
- CPU通常以固定大小的块（4字节、8字节）访问内存，如果未对齐，我们要用访问两次内存的值才能拼接成结果

### 僵尸进程与孤儿进程

- 僵尸进程：进程已终止后，但父进程尚未调用`wait()`或`waitpid()`获取其退出状态，此时进程描述符（PCB）仍保留在系统中，成为僵尸进程。父进程终止后，僵尸进程会被 init 接管并清除。
- 孤儿进程：父进程先于子进程终止，导致子进程成为 “孤儿”，此时子进程会被**init 进程（PID=1）或 systemd 进程**收养。

### 父子进程共享什么资源

- 代码段
- 文件描述符
- 信号处理函数
- 继承父进程的权限

### 虚拟内存

​	**虚拟内存** 是计算机系统内存管理的一种技术。它为每个进程提供了一个连续、私有且一致的地址空间，让进程以为自己拥有了一整块巨大的内存，而实际上这只是硬件异常、主存地址翻译和磁盘文件的完美结合。

它解决了什么问题？

1. 内存容量限制：允许运行比物理内存容量更大的程序。系统可以将暂时不用的数据置换到磁盘，只将活跃数据留在物理内存。
2. 内存碎片：物理内存往往是破碎的。虚拟内存通过页表映射，可以将不连续的物理空间映射为连续的虚拟空间，简化了编程。
3. 安全性与隔离：每个进程都有独立的虚拟地址空间。一个进程无法直接访问或修改另一个进程的内存，防止了非法访问和程序崩溃相互影响。

### 内部碎片与外部碎片

- 内部碎片：当内存分配的空间大于进程实际需要的空间时，多余的部分无法被其他进程使用，从而形成 “内部” 的空闲碎片。
- 外部碎片：内存中存在多个分散的空闲内存块，但它们的总大小足够满足进程需求，却因不连续而无法分配

### 用户态与核心态

- ⽤户态：在⽤户态下，进程或程序只能访问受限的资源和执⾏受限的指令集，不能直接访问操作系统的核⼼部分，也不能直接访问硬件资源。
- 核⼼态：核⼼态是操作系统的特权级别，允许进程或程序执⾏特权指令和访问操作系统的核⼼部分。在核⼼态下，进程可以直接访问硬件资源，执⾏系统调⽤，管理内存、⽂件系统等操作。
- 发生系统调用、异常、中断时、从用户态切换到核心态

### 死锁

- 互斥条件：⼀个进程占⽤了某个资源时，其他进程⽆法同时占⽤该资源。
- 请求与保持条件：⼀个线程因为请求资源⽽阻塞的时候，不会释放⾃⼰的资源。
- 不可剥夺条件：资源不能被强制性地从⼀个进程中剥夺，只能由持有者⾃愿释放。
- 循环等待：多个进程之间形成⼀个循环等待资源的链  

预防死锁：

- 破坏请求与保持条件：⼀次性申请所有的资源。
- 破坏不可剥夺条件：占⽤部分资源的线程进⼀步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
- 破坏循环等待条件：靠按序申请资源来预防。让所有进程按照相同的顺序请求资源，释放资源则反序释放。

避免死锁：

- 银行家算法

## ----数据库----

### MySQL索引

索引是一种数据结构，用于提高数据库表中数据的查询效率。它就像一本书的目录，通过特定的键值来快速定位到相应的数据行。

**作用**：能大大减少数据库在查询时需要扫描的数据量，从而加快查询速度。例如在一个有大量记录的用户表中，若经常根据用户名查询用户信息，为用户名字段建立索引，查询时就可直接定位到对应记录，而无需全表扫描。

**类型**：

- 数据结构角度：常见的有 B - Tree 索引、哈希索引等。B - Tree 索引适用于范围查询和排序操作，哈希索引则在等值查询时性能出色。
- 物理存储角度：聚集索引 -- 数据与索引一起存放，找到了索引就找到了数据。 非聚集索引 -- 数据存储与索引分开存放
- 逻辑角度：
  - 主键索引 -- 不允许有空值，特殊的唯一索引
  - 唯一索引 -- 索引列中的值必须是唯一的，可以为空值
  - 联合索引 -- 多个字段创建的索引， 使用时遵循 最左前缀原则

### 为什么使用B+树做索引

- B+ 树的⾮叶⼦节点不存放实际的记录数据，仅存放索引，数据量相同的情况下，B+树的⾮叶⼦节点可以存放更多的索引，整个树的高度会矮一点，查询底层节点的磁盘 I/O次数会更少。  
- B+ 树所有叶⼦节点间有⼀个链表进⾏连接，范围查询效率高

### 什么时候需要创建索引

- 表的主键字段，会自动建立唯一索引
- 经常用于WHERE查询条件的字段
- 查询中排序的字段
- 与其他表有关联的字段

### 索引失效的场景

- 对索引使用函数
- 对索引使用左 或者 左右模糊匹配
- 对索引进行表达式计算
- 对索引隐式类型转换
- OR 前的条件列是索引列，而在 OR 后的条件列不是索引列

### MySQL事务

事务是由一组 SQL 语句组成的逻辑单元，这些语句要么全部执行成功，要么全部不执行，以保证数据库的一致性。

- **特性**：具有原子性（通过undo log）、一致性、隔离性（通过MVCC，多版本并发控制）和持久性（通过redo log），即 ACID 特性。原子性确保事务中的操作要么都完成，要么都不做；一致性保证事务执行前后数据库的完整性约束得到满足；隔离性防止并发事务之间相互干扰；持久性保证事务一旦提交，其结果就永久保存在数据库中。
- 事务的隔离级别
  - 读未提交 -- 允许事务读取其他事务尚未提交的数据。--  会出现脏读，即一个事务读取到了另一个未提交事务修改的数据
  - 读提交 -- 只能读取已经提交的数据，避免了脏读。--  在一个事务内的多次查询可能会出现不可重复读的情况，即由于其他事务在该事务两次查询之间对数据进行了修改并提交，导致两次查询结果不一致。
  - 可重复读 -- 确保在一个事务内多次读取同一数据时，结果是一致的，解决了不可重复读问题。-- 可能会出现幻读，在一个事务内多次查询某个符合查询条件的「记录数量」，出现前后两次查询到的记录数量不一样的情况
  - 串行化 -- 事务串行执行，就像没有并发一样，避免了所有并发问题。-- 性能开销大

### MVCC机制

⽤于管理多个事务同时访问和修改数据库的数据，⽽不会导致数据不⼀致或冲突。MVCC的核⼼思想是每个事务在数据库中看到的数据版本是事务开始时的⼀个快照，⽽不是实际的最新版本。这使得多个事务可以并发执⾏，⽽不会互相⼲扰 

快照包含四个信息：

- m_ids 当前活跃的所有事务id（所有未提交的事务）
- min_trx_id 当前活跃事务中id最小的，事务id比这个小就代表，再快照开启前，该事务已经提交
- max_trx_id 下⼀个将要分配的事务id（版本链头事务id+1），如果一条记录的事务id**大于等于**这个就代表，快照创建后，该事务才开启，不能访问。（可以查找undo log寻找上一个修改该记录的事务）
- creator_trx_id 创建这个ReadView的事务的id 查询规则

### MySQL锁机制

锁是数据库管理系统用于控制并发访问的一种机制，它允许在同一时间内对特定的数据资源进行排他性或共享性的访问控制。

- **从操作类型上**可分为共享锁（读锁）和排他锁（写锁）。共享锁允许事务对数据进行读操作，多个事务可同时获取共享锁来读取数据；排他锁则用于写操作，一个事务获取排他锁后，其他事务不能再获取该数据的任何锁，直到排他锁被释放。
- **从粒度上**可分为表级锁、行级锁等。表级锁对整个表进行锁定，开销小但并发度低；行级锁只对特定行进行锁定，能支持更高的并发，但开销较大。
- 行锁分为：
  - 记录锁
  - 间隙锁
  - Next-Key Lock，锁定一个范围，并锁定记录本身

### Redis缓存的穿透、击穿、雪崩

1. 缓存穿透 —— “查不存在的数据”
    定义：请求的数据在缓存中没有，在数据库中也没有。由于数据库没查到，所以也就不会写入缓存。这导致每次对该数据的请求都会直接打到数据库。
  - 场景：恶意攻击（黑客用不存在的非法 ID 大规模请求）或业务逻辑错误。
  - 后果：数据库压力剧增，甚至被冲垮。

​	应对方案：

- 缓存空对象：如果数据库查不到，就在缓存里存一个 nil 或特殊字符串，并设置一个较短的过期时间。
- 布隆过滤器 (Bloom Filter)：在缓存前加一层“过滤器”。将所有可能存在的 Key 存入布隆过滤器。如果过滤器判断 Key 不存在，则直接拦截，不再查询数据库。

2. 缓存击穿 —— “热点 Key 突然过期”
    定义：某个被超高并发访问的“热点数据”，在过期的瞬间，海量请求同时涌入。由于缓存失效，所有请求都会尝试去查询数据库并重建缓存。
  - 场景：微博热搜话题、秒杀活动中的商品信息突然到期。
  - 后果：数据库瞬时压力过载，产生卡顿或宕机。

​	应对方案：

- 设置热点数据永不过期：物理上不设置过期时间，而是由后台异步线程负责更新。
- 互斥锁 (Mutex Lock)：只允许第一个请求去查询数据库并回写缓存，其他请求等待（自旋）或重试。在 Redis 中常用 SETNX 实现。

3. 缓存雪崩 (Cache Avalanche) —— “大面积 Key 同时过期”
    定义：在极短的时间内，大量的缓存 Key 同时失效，或者缓存服务器（Redis）直接宕机。此时原本由缓存承担的所有流量瞬间全部涌向数据库。
  - 场景：设置了相同的过期时间（如凌晨 0 点全部失效），或 Redis 集群崩溃。
  - 后果：数据库因无法承受海量流量而迅速崩溃，引发连锁反应（雪崩）。

​	应对方案：

- 随机化过期时间 (Jitter)：给每个 Key 的过期时间加上一个随机抖动值（如 5-10 分钟），防止它们在同一秒失效。
- 搭建高可用缓存集群：使用 Redis Sentinel（哨兵）或 Cluster 模式，避免单点故障。
- 多级缓存：使用本地缓存（如 Caffeine / Guava）+ 远程缓存（Redis）。
- 服务熔断与限流：如果数据库实在顶不住，开启熔断，暂时牺牲部分用户请求，保住整体核心业务。

### Redis持久化

- AOF ⽇志：每执⾏⼀条写操作命令，就把该命令以追加的⽅式写⼊到⼀个⽂件⾥。Redis刚启动时，会读取该日志来构建数据库
- RDB 快照：将某⼀时刻的内存数据，以⼆进制的⽅式写⼊磁盘；每次执⾏快照，都是把内存中的「**所有数据**」都记录到磁盘中  
- 混合持久化⽅式： Redis 4.0 新增的⽅式，集成了 AOF 和 RBD 的优点；

## ----设计模式----

### 单例模式

```C++
class Singleton {
public:
    // 删除拷贝构造函数和赋值操作符
    Singleton(const Singleton&) = delete;
    Singleton& operator=(const Singleton&) = delete;

    // 全局访问点
    static Singleton& getInstance() {
        static Singleton instance; // C++11 保证静态局部变量初始化线程安全
        return instance;
    }

private:
    Singleton() {} // 私有构造函数，使得只有类内部可创建实例
    ~Singleton() {} // 私有析构函数
};
```

双重检查锁定

```C++
class Singleton {
public:
    // 删除拷贝构造函数和赋值操作符
    Singleton(const Singleton&) = delete;
    Singleton& operator=(const Singleton&) = delete;

    // 全局访问点
    static Singleton* getInstance() {
        if (instance == nullptr)
        {
            std::lock_guard<std::mutex> lock(mutex_);
            if (instance == nullptr)
            {
                instance = new Singleton();
            }
        }
        return instance;
    }

private:
    Singleton() {} // 私有构造函数
    ~Singleton() {} // 私有析构函数
    static Singleton *instance;
    static std::mutex mutex_; // 保证无论通过多少个线程调用getInstance()，始终操作同一个mutex_
};

// 静态成员变量需要在类外初始化
Singleton::instance = nullptr;
std::mutex Singleton::mutex_;
```



### 工厂模式

将对象的创建逻辑封装在一个工厂类或工厂方法中，使代码与具体对象的依赖关系解耦

- **解耦对象的创建与使用**，通过统一接口创建不同类型对象
- 简单工厂 --  一个工厂类根据工厂函数的参数创建不同产品
- 工厂方法 --  抽象工厂定义一个创建对象的抽象方法，由具体工厂子类实现该方法来创建产品。
- 抽象工厂 --  工厂基类定义一个创建一系列相关产品的抽象接口，具体工厂（一系列相关产品的不同生厂商）实现该接口来创建产品族。







### 水平触发与边缘触发

- 水平触发：当文件描述符对应的 IO 缓冲区中存在未读取的数据（读事件）或缓冲区未满（写事件）时，会持续触发事件通知。支持阻塞 / 非阻塞 IO

- 边缘触发：当文件描述符的 IO 状态发生变化时（如从不可读变为可读），仅触发一次事件通知，后续状态持续就绪时不再触发。必须使用非阻塞 IO

- 边缘触发一定要非阻塞IO：
  - 假设在 ET 模式下使用阻塞 IO
  - 若数据未读完，阻塞 IO 会等待后续数据，导致线程卡住；
  - 但由于 ET 模式不会再次触发事件，线程将永久阻塞，导致程序卡死

- 阻塞IO：当进程发起 IO 操作时，如果数据未就绪，进程会被挂起（进入等待状态），直到数据就绪并完成操作后才继续执行。阻塞 IO 会导致当前线程暂停执行，无法处理其他任务，直到 IO 完成

- 非阻塞IO：当进程发起 IO 操作时，如果数据未就绪，系统调用会立即返回（通常返回`EWOULDBLOCK`或`EAGAIN`错误），进程可以继续执行其他任务。进程需主动调用`read()`/`write()`尝试操作，若返回错误则继续执行其他任务，稍后再次尝试。

  

### protobuf

Protocol Buffers是 Google 开发的一种高效、跨平台、语言无关的数据序列化协议，用于结构化数据的编解码。它通过预定义的消息格式（`.proto` 文件）生成代码，广泛应用于 RPC 框架、数据存储、配置文件等场景。

Json与protobuf的对比：

- protobuf将数据编码成二进制格式，Json则是文本格式
- Json可直接读
- protobuf数据体积小很多
- 都是跨语言支持的

protobuf与json的性能差异原因：

- 数据格式的差异，json基于文本，解析时需要逐字符扫描，protobuf的二进制编码直接按字节流处理
- json是动态结构，每次解析需要实时推断类型，protobuf通过.proto文件预定义类型
- protobuf支持零拷贝解析，直接将字节流按照预定义的结构填充到内存中。

- 

- 

## ----云服务和分布式----

### 服务发现

在动态变化的分布式环境中，服务 A 怎么找到服务 B 的 IP 地址和端口？

由于云原生环境下 Pod/实例的 IP 经常因缩容、宕机或发布而改变，硬编码（Hardcoding）IP 是不可能的。服务发现主要通过 “注册中心” 来协调。
服务发现通常包含以下三个关键步骤：

- 服务注册 (Registration)：服务实例启动时，将自己的服务名、IP、端口等信息上报给注册中心。
- 服务续约 (Heartbeat)：服务通过心跳维持在线状态。如果心跳停止，注册中心会将其剔除。
- 服务发现 (Discovery)：调用方（客户端）询问注册中心：“请给我服务 B 的所有可用地址”，注册中心返回列表。

1. 微服务架构中，服务发现是如何实现的? 

### 熔断、降级、限流

1. 限流  —— “控制入口”
    限流是指在系统的入口处，对请求的速率进行限制。

  解决的问题：流量激增（暴涨）。比如遭受恶意攻击、突发的热点新闻、或者秒杀活动。如果不限流，瞬间的高并发会直接压垮数据库或 CPU。

  做法：设置一个阈值（如每秒只允许 1000 个请求）。超过这个速率的请求会被直接丢弃、排队等待或转到错误页面。

  常用算法：

  - 令牌桶算法（允许一定程度的突发流量）。
  - 漏桶算法（强行平滑流量）。

2. 熔断 —— “及时止损”
    当某个下游服务（依赖的服务）出现大量错误或超时时，上游服务为了保护自己，会主动断开对该服务的调用。

  解决的问题：服务雪崩。防止故障在分布式系统中蔓延。如果服务 A 调用服务 B，B 变慢了，A 的线程就会被卡住，进而导致调用 A 的服务 C 也卡住，最终全线崩溃。

  三种状态：

  - Closed（关闭）：正常状态，流量通过。
  - Open（开启）：错误率达标，直接熔断，所有请求立即返回失败，不调用后端。
  - Half-Open（半开）：经过一段时间后，尝试放行少量请求看后端是否恢复。

3. 降级 —— “舍鱼而取熊掌”
    降级是指当系统负载过高或某个非核心功能出故障时，暂时关闭这些功能，把有限的资源留给最重要的业务。

  解决的问题：资源不足或局部服务不可用。确保核心业务流程（如付款）依然能跑通，牺牲掉非核心体验（如推荐位、评论区）。

  做法：

  - 返回默认值：请求失败时返回空列表或缓存数据。
  - 页面降级：App 上某些模块直接显示“服务维护中”。
  - 延迟处理：将同步操作转为后台异步执行（如先发货，晚点再发通知短信）。

### 消息队列

消息队列主要扮演以下三个关键角色：

1. 解耦 (Decoupling)：
   - 问题：如果系统 A 直接调用系统 B，当 B 接口变动或 B 宕机时，A 也会受影响。
   - 解决：A 只需把消息发到队列，不用关心谁来消费、怎么消费。即使 B 挂了，等它恢复后依然能从队列拿到数据。
2. 异步处理 (Asynchrony)：
   - 问题：用户注册后，系统需要发邮件、送积分、通知风控。如果同步执行，响应时间会极长。
   - 解决：主流程完成后立即给用户返回成功，耗时的非核心逻辑（发邮件等）写入消息队列，由后台服务慢慢处理。

3. 削峰填谷 (Buffering/Flow Control)：
   - 问题：秒杀活动时，瞬时流量（如 10万/秒）会直接冲垮数据库。
   - 解决：消息队列像一个巨大的“蓄水池”，把海量请求缓存起来。后端服务根据自己的处理能力（如 1千/秒），平滑地从队列中拉取数据，保证系统不崩溃。

### Kafka

Kafka 能够实现超高吞吐量的关键，就在于它的“分区”和“消费者组”设计。

1. 分区 —— 物理上的“并行”
    Kafka 的 Topic（主题） 并不是一个巨大的文件，而是被切分成了多个物理上的 分区。
  - 为什么要分区？
    - 水平扩展：不同的分区可以分布在不同的服务器（Broker）上，突破单机的存储和带宽限制。
    - 提高并发：分区是 Kafka 并行处理的最小单位。多个分区可以由不同的消费者同时读取。
  - 顺序保证：Kafka 只保证在同一个分区内消息是有序的，不保证整个 Topic 级别的全局有序。

2. 消费者组 —— 逻辑上的“分工”
    消费者组是 Kafka 实现发布/订阅模型和负载均衡的核心机制。
  - 负载均衡模式：一个 Topic 的消息会被分发给组内的不同消费者。同一个分区在同一时刻只能被同一个消费者组内的一个消费者消费。
    - 例子：Topic 有 4 个分区，组内有 2 个消费者。那么每个消费者负责处理 2 个分区的消息。
  - 广播模式：如果你想让多个不同的业务系统都收到同一份消息，只需要让它们属于不同的消费者组即可。每个组都会收到一份完整的消息副本。
